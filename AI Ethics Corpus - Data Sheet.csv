,,,,,,
#,Title,Article Link,Contributor,Type,Notes,
1,Building trust in the age of AI: A balancing act of ethics and innovation,https://business.inquirer.net/468719/building-trust-in-the-age-of-ai-a-balancing-act-of-ethics-and-innovation-2,Dan,News Article,,
2,Advancing AI Ethics in BangladeshAdvancing AI Ethics in Bangladesh,https://www.unesco.org/en/articles/advancing-ai-ethics-bangladesh,Awais,Industry Article,"Advancing AI Ethics in Bangladesh
Artificial Intelligence (AI) has become essential in our daily lives, influencing sectors such as healthcare, education, and governance. However, with its immense capabilities comes the responsibility to use it sensibly.


Ensuring that AI is developed and deployed ethically is crucial to prevent harm and ensure it benefits society positively. Recognizing this importance, UNESCO (United Nations Educational, Scientific and Cultural Organization) has taken significant steps to address AI ethics through its Recommendation on the Ethics of AI, which provides guidelines to promote the responsible use of AI, safeguarding human rights, and promoting societal well-being.

The Readiness Assessment Methodology (RAM)
The Readiness Assessment Methodology (RAM) is a key tool through which UNESCO is supporting the Member States to evaluate a country’s preparedness for implementing AI. The RAM focuses on multiple dimensions of readiness, including areas such as technological infrastructure, legislation, and the economy, with a focus throughout on ethical considerations and effective governance. The Readiness Assessment Methodology goes beyond other readiness assessment tools by combining both quantitative and qualitative questions. It provides detailed insights into institutional and regulatory gaps, allowing UNESCO to tailor support for governments to address these gaps effectively. Recognizing UNESCO’s role and efforts in steering global AI ethics and governance, UNESCO and the RAM was featured in the Oxford Insights Government AI Readiness Index 2023.
Implementation in Bangladesh
In 2024, UNESCO, in collaboration with the ICT Ministry and Aspire to Innovate (a2i) will conduct the Readiness Assessment Methodology in Bangladesh. The primary goal is to conduct the Readiness Assessment Methodology and develop a comprehensive report with data-driven insights and key recommendations, helping policymakers to make informed decisions on the ethical development, design, and deployment of AI technologies. This will help support Bangladesh in creating an ethical AI ecosystem aligned with the global standards. 
The Readiness Assessment Methodology is set to have an important impact on driving responsible AI practices on a global scale. By assessing readiness and identifying gaps, it empowers countries to build AI systems that serves the common good while upholding human rights and human dignity. ",
3,Global leaders convene in Hiroshima to chart ethical AI Pathways for peace,https://www.thecitizen.co.tz/tanzania/news/national/global-leaders-convene-in-hiroshima-to-chart-ethical-ai-pathways-for-peace-4686958,Dan,News Article,,
4,IBM reaffirms its commitment to the Rome Call for AI ethics,https://research.ibm.com/blog/ibm-ai-ethics-japan-rome-call,Dan,Industry Article,,
5,WHO releases AI ethics and governance guidance for large multi-modal models,https://www.who.int/news/item/18-01-2024-who-releases-ai-ethics-and-governance-guidance-for-large-multi-modal-models,Awais,Industry Article,,
6,"In Hiroshima, a call for peaceful, ethical AI",https://newsroom.cisco.com/c/r/newsroom/en/us/a/y2024/m07/in-hiroshima-a-call-for-peaceful-ethical-ai.html,Awais,Industry Article,,
7,Generative AI: 8 biggest concerns and risks,https://www.techtarget.com/searchenterpriseai/tip/Generative-AI-ethics-8-biggest-concerns,Rhea,News Article,,
8,IBM Recommits to the 'Rome Call for AI Ethics' as the Vatican Welcomes Eastern Religions to the Pledge,https://newsroom.ibm.com/2024-07-10-IBM-Recommits-to-the-Rome-Call-for-AI-Ethics-as-the-Vatican-Welcomes-Eastern-Religions-to-the-Pledge,Awais,News Article,,
9,How businesses can responsibly use AI and address ethical and security challenges,https://legal.thomsonreuters.com/blog/how-to-responsibly-use-ai-to-address-ethical-and-risk-challenges/,Ned,News Article,"How businesses can responsibly use AI and address ethical and security challenges


Addressing ethical and security concerns in AI usage, emphasizing transparency, accountability, and responsible governance frameworks.

In our Future of Professionals report, we address the many concerns and challenges that are being raised around the use and application of AI in professional and business capacities. Legal and accounting businesses are faced with figuring out how to use AI ethically and responsibly. Thomson Reuters has been a thought leader in developing principles to guide practices and helping our clients navigate the complexity with AI for decades. As generative artificial intelligence continues to infiltrate our lives personally and professionally, we strive to provide the most up-to-date research and best practices on leveraging these technologies with accountability and transparency. 

Ethical and security issues of AI
While many legal and accounting firms are trying to determine their AI practices and governance, there are a number of issues that need to be addressed. Older generations are being challenged to adapt to the changes the incorporation of AI into professional services is having and may offer resistance from a fear that AI will push ethics out the window. While younger generations are looking at the possibilities generative AI opens. However, there seems to be little debate over the fact that businesses that embrace AI will be the ones to get ahead. 

Across many industries, 15% of professionals reported data security and ethics as their biggest fear, with a lack of transparency and accountability close behind. The appeal of AI is that it can assist with numerous repetitive, time-consuming activities and tasks within the operations of a professional firm or department. It has also been found to improve mental health by relieving anxiety, isolation, and burnout by creating more tool-based capacity for this type of work increasing the amount of time professionals have to nurture client relationships and grow their client base.  


While there are other fears around accuracy and loss of jobs, data security and ethics make a combined 30%.
As much as AI can help businesses and departments do beneficial work, there are ethical considerations to take into account. AI can help fraudsters conduct their activities more efficiently and accurately. It is also important to always remember that AI generates responses based on algorithms created by humans and information provided by humans. As such, humans should be held accountable for verifying and fact-checking what AI generates.  

The eagerness to leverage AI has created some situations that have highlighted the necessity of consent and accountability, including cases where AI has “hallucinated” an answer – meaning it improvised a response based on the information it had – which led to legal and ethical consequences. Since AI does not provide any data on the source of the responses, ensuring a human validation process should be a part of AI policies. AI is also subject to bias risk mitigation, in which the data used to train a model may result in errors favoring one outcome over another. This can result in a lack of trust and fairness.  

 

Future of Professionals Report: How AI is the catalyst for transforming every aspect of work        
Future of Professionals Report
How AI is the Catalyst for Transforming Every Aspect of Work

View report
 

 

Using AI responsibly
The United States has issued an Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence that establishes principles and guidelines for federal agencies to follow when implementing an AI system. It also offers a framework for working with various stakeholders. The Future of Professionals report shows that 52% of professionals believe that regulations governing the professional ethics of AI are a necessity, and 25% believe that governments should be designing and overseeing professional ethics regulations. 


Overall, over 75% of professionals heavily believe the profession and the government should regulate AI.
Key areas of AI governance frameworks should cover encryption and authentication protocols, frequent auditing and testing procedures, traceability, educating employees on the proper and ethical use of AI, and best practices for securing and protecting confidential data. Ultimately, businesses and departments should be able to understand how an algorithm arrives at its output and where the data comes from. Because it pulls data from large data sets, those data sets should be legitimate and qualified to get the most accurate and relevant output.  

If AI is to be incorporated into your workflow, a human-centric design should be at the core of the initiative. Transparency and accountability are all crucial to maintaining trust with clients, users, and employees. Biases should be avoided, fairness should be promoted, and security should be the top priority.  

Given the private and confidential nature of the information that professionals are working with, we believe that developing internal AI regulations and governance at the firm level is critical to establishing trust, accountability, and transparency around the consent and privacy of client information. It’s important to understand the principles behind AI ethics and make sure your organization has a framework that is tailored to the business and your customers. 

Confidence in your adoption of AI for all stakeholders lies in professionals’ ability to provide high levels of service, access to information on the use of AI, and foster a culture of risk mitigation and awareness. For more information, download the Future of Professionals report.  ",
10,"MUSCAT: The Islamic World AI Charter (IWAIC) Regional Workshop, which kicked off on Tuesday in Muscat and held under the auspices of Dr Khamis bin Said al Jabri, Chairman of Oman Vision 2040 Implementation Follow-up Unit, underscored the urgency and strategies for adopting ethical AI practices across member states.... Read more on: https://www.omanobserver.om/article/1155842/business/economy/united-front-asia-and-the-middle-east-push-for-ethical-ai",https://www.omanobserver.om/article/1155842/business/economy/united-front-asia-and-the-middle-east-push-for-ethical-ai,Dan,News Article,,
11,AI ethics and values,https://www.philstar.com/business/2024/06/30/2366495/ai-ethics-and-values,Dan,News Article,,
12,Making AI great again: how do we design ethical AI systems?,https://www.wired.com/sponsored/story/making-ai-great-again-how-do-we-design-ethical-ai-systems/,Ned,News Article,"Making AI great again: how do we design ethical AI systems?
The debate around how AI will shape our future is ongoing, and it is already affecting our present. Accenture investigates how issues around ethics and regulation are becoming increasingly urgent.

Making AI great again how do we design ethical AI systems
ARTIFICIAL INTELLIGENCE IS a powerful agent of change in our society. The debate around how AI will shape our future is ongoing, but one thing is certain: it is already affecting our present – altering the way we work, entertain ourselves and interact with others. As this technology catches on, changing the way we understand ourselves and our society, issues around ethics and regulation are becoming more and more pressing – and increasingly debated.

As designers and developers of AI systems, we need to bridge the gap between data technology and human experience. Addressing fairness, accountability and the long-term effects on our society when designing with data will allow us to contribute to the society we want, fostering higher levels of cognitive and emotional skills.

Designing collaboration between people and AI
With advances in AI technology, even jobs once regarded as necessarily requiring human knowledge and experience – doctors, lawyers, teachers, financial advisers, business consultants – will end up being affected by automation.

It therefore becomes crucial to identify the best ways to work with machines that advance our cognitive and emotional skills. We already know that in combination, people and machines are more effective than either alone. The 2016 White House report on preparing for the future of AI highlighted a study comparing human and machine in relation to diagnosing cancerous lymph nodes. The AI “doctor” had an error rate of 7.5 per cent; the human had an error rate of 3.5 per cent. Together, though, their error rate fell to just 0.5 per cent.

This suggests that, regardless of widespread automation of data collection and analysis processes, we still need that human touch. A doctor working with AI might be able to diagnose more accurately than ever. And, importantly, only a flesh-and-blood doctor can show the compassion and understanding patients crave when receiving a diagnosis or discussing treatment options.

Research into emotionally aware AI is underway, and eventually we will have systems that can sense whether we are worried or angry, and configure their responses appropriately. When cyborg systems become reality instead of fiction, with humans and machines working side by side, we need to keep in mind that there are intangible benefits to our interactions with other humans. But, just because we can create an AI doctor with a tolerable bedside manner, does it mean we should?

In a way, automation may ultimately help us become more human. We will begin to place more value on characteristics such as empathy and social awareness, as opposed to analytical thinking. Our education and training may change to reflect this, with focus shifting to communication and how we interact with others as opposed to the computer-based analytical work that is often the norm today.

A shared responsibility – to society and ourselves
AI systems are intrinsically tied to human biases; you may have already heard the highly publicised cases of AI systems reflecting our ugly human prejudices, from Google Photos’ racially biased tagging system to Microsoft Tay’s offensive behaviour on Twitter. We know that we need to design our systems to account for human biases in data and be more explainable to facilitate trust. That is not only needed to avoid another PR nightmare for large corporations; it is about designing AI that reflects and evolves with our vision for a fairer and more ethical society.

In the design world, we talk about being human-centric – researching and interviewing our users to create services that address their true needs. However, looking back at the changes AI has brought about in our lives, our processes need to shift from a traditional individual user-centric approach to include humanity and society on a larger scale. We’re designing our AI systems for an ideal future world – what should that look like, and what are the problems we’ll encounter trying to get there?

A new way of talking about AI
Designing AI in line with ethical behaviour and societal responsibility in mind is only part of the solution – we also need to change the way we talk about AI.

Paradoxically, we perceive AI systems as cold and lacking in humanity. Even here I’ve talked about needing a more humanity-centred approach to AI – but when something goes wrong, we default to blaming the system, framing AI in a very human way. The media suggests that AI is “out to get us”, and systems have been dubbed as racist and sexist – two inherently human traits. This deflection of responsibility is dangerous, and suggesting that an unconscious system has evil intentions is irresponsible and scare-mongering.

We need to stop framing AI as an evil genius that will take our jobs and eventually ruin humankind, and start talking about it in a way that reflects reality as it is: AI is simply a tool. If a light bulb exploded, we wouldn’t say it was “out to get us”, deferring responsibility from people and putting it on the once very novel technology of electricity. It is simply an exploding light bulb, and there is no reason we should talk about AI technology differently. We need a change in tone aimed at promoting a mindset of accountability for what we create. This, along with actively applying frameworks for responsibility when designing AI applications to be used by businesses or governments, will help us place responsibility back on the people in the system.

Technology should be a positive force in our lives, enabling us to do things that were once unimaginable and making us better humans. Earlier this month at Web Summit, Stephen Hawking commented that the rise of AI could be “the worst or the best thing that has happened for humanity.” I think we can all agree: it is our responsibility – as designers, developers, governments, businesses and, ultimately, humans and members of society – to ensure it is the latter.

Jivan Virdee is a data designer at Fjord, with a background in data science and an interest in building adaptable, human-centric data-driven experiences

--

During WIRED Live 2017, Accenture and Fjord ran a Live Innovation looking at the ethics of AI and how we can address the consequences of badly designed AI, to co-create an AI ethics manifesto. Take a look at the result here.

This article was originally published by WIRED UK",
13,A pink slime site used AI to rewrite our AI ethics article,https://www.poynter.org/ethics-trust/2024/a-pink-slime-site-used-ai-to-rewrite-our-ai-ethics-article/,Ned,News Article,"A pink slime site used AI to rewrite our AI ethics article
Even Poynter’s guide for using generative AI ethically isn’t immune from those who won’t.
   
Hours after Poynter released its AI ethics guide, a near-identical article appeared on a sketchy website. It was likely written by artificial intelligence.

The piece, filed in Tech Gate by an alleged human named Bourbiza Mohamed — who racks up bylines on stories about video games, Bitcoin and NASA every five minutes or so — was published four hours after Poynter’s story. It had Poynter’s art and logo, and followed the same structure as Kelly McBride’s article, but nearly every sentence was rewritten with peculiar word choices.

For example, the Tech Gate article says:

“Consider of it (sic) like a meals prep package. A lot of the function is completed, however you even now must roll up your sleeves and perform a little little (sic) bit of labor.”

Compare that to McBride’s piece:

“Think of it like a meal prep kit. Most of the work is done, but you still have to roll up your sleeves and do a bit of labor.”

Tech Gate’s about me page is vague and lacks any contact information. It says the site launched in 2007, but the site’s earliest archive in the Wayback Machine is from 2015 — and is written in Arabic.

Even an article about the ethical use of AI isn’t immune from bad actors who will use the technology unethically. In this case, the site likely steals content for easy advertising revenue.

Tech Gate is a pink slime news site, a website masquerading as a news source that is filled with poor-quality reporting (usually no reporting) or AI-generated articles and that may be used by political operatives to launder opinion pieces.

Last year, I spoke with Charlie Melvin, publisher of the Richmond Observer, who showed me a pink slime site doing the same thing with his organization’s local stories. He worried that at scale, AI would cheapen the value of original content — since it could be easily plagiarized on networks of sketchy websites.

This episode highlights a few challenges for news outlets. One: How can you compete with unethical outlets in the age of generative AI? And two: How do I protect my intellectual property?

While I can’t fully answer either, I can say that this curious case shows that AI will be used increasingly to feed the online content beast — ethically or not. The only way you can compete is by experimenting with and implementing generative AI in your newsroom. AI is coming for your content, and this industry.

I hope Poynter’s AI ethics guide — and not Tech Gate’s Dollar Store knockoff — can expand your shop’s resources to fight back against the rise of AI slime.",
14,6 Critical – And Urgent – Ethics Issues With AI,https://www.forbes.com/sites/eliamdur/2024/01/24/6-critical--and-urgent--ethics-issues-with-ai/,Rhea,News Article,"6 Critical – And Urgent – Ethics Issues With AI
Eli Amdur
Contributor
Leadership professor, job market journalist-analyst, business advisor.
Jan 24, 2024,11:50am EST
Code of Ethics in Technology as a Business ConceptGETTY
There is no doubt in most people’s minds that AI is one of the most transformative technologies of our time. There is no doubt in mine that it is the most transformative technology ever.

The “of our time” position has been borne out already. By the end of 2024, my “ever” position will likely be validated as well.

AI has lost no time in unfolding its immense potential to improve efficiency, aid research and discovery, enhance decision-making, and solve complex problems – all at a blinding rate of acceleration. It is precisely this rate of acceleration that’s propelling the future towards us faster than ever before, the reason I believe that will lead us to the conclusion this year that AI will have surpassed all other advances ever made.

The cold, hard truth
Given AI’s potential to be a more powerful positive force than any other in history, it stands to reason that it will have the same possibility on the negative side of the equation, as it’s been with every other invention or discovery ever. This also raises significant ethical concerns that demand our attention and thoughtful consideration.

So I did an informal poll of six experts with lots of AI experience – as practitioners and as academics – and asked them, simply, what we should be worried about. How, I asked them, should we and could we ensure that AI systems act ethically? On six issues, here’s their collective thinking.

MORE FROMFORBES ADVISOR
Best High-Yield Savings Accounts Of September 2023
ByKevin PayneContributor
Best 5% Interest Savings Accounts of September 2023
ByCassidy HortonContributor
1. Data Bias
One of the foremost ethical concerns surrounding AI is data bias. AI systems are only as good as the data they’re trained on, so objective data curation becomes paramount. Thus, developers and researchers must prioritize and standardize rigorous testing and continuous monitoring.

2. Privacy
As AI systems become more sophisticated and far reaching in data collection and analysis, the line between security and surveillance blurs. From facial recognition to smart home devices, the potential for invasions of privacy, not to mention election tampering and corporate hacking, is ominous.

3. Accountability
As AI systems make more decisions that impact our lives, it becomes more critical to establish clear lines of responsibility. Who should be held accountable when an autonomous vehicle makes a mistake? Or when healthcare diagnoses are made and decisions about medications or therapies are carried out? This applies in the legal arena, too.

4. Job Displacement
New technologies inevitably lead to job losses in old industries but even greater job creation in new ones. Seamless transition from old to new industries depends on a four-part coalition among: the individuals who need the jobs, the employers who will offer them, higher education that will develop a skilled workforce, and government that will fund this. At the same time, must be remembered that, over the last three years, progress has been made to shrink the income and wealth gap, ground tht has been gained and that cannot be relinquished. This is nothing less than an issue of national interest.

5. Transparency
All AI stakeholders – producers, educators, users, and casual observers – deserve to have a clear understanding of how AI systems make decisions. Algorithms are just as easily sinister as they are life supporting, making scrutiny a key factor.

6. What’s Ahead?
We’re nowhere near the development of superintelligent AI – the 800-pound gorilla in the room – but we’re closer than we think, as the rate of acceleration comes into play. As we move closer to creating AI systems that surpass human intelligence, questions about their control and alignment with human values come to the fore. If you haven’t seen the film 2001: A Space Odyssey yet, do not wait any longer. Safeguards must be in place to prevent AI from evolving in ways that could threaten humanity. We can start by defaulting to Isaac Asimov’s Three Laws of Robotics, a good place for me to end this essay and implore you to look it up and continue from there.",
15,Google Splits Up a Key AI Ethics Watchdog,https://www.wired.com/story/google-splits-up-responsible-innovation-ai-team/,Ned,News Article,"Google Splits Up a Key AI Ethics Watchdog
A crucial team at Google that reviewed new AI products for compliance with its rules for responsible AI development faces an uncertain future after its leader departed this month.
Red green yellow and blue hexagonal wooden blocks in a messy pile on a yellow background
PHOTOGRAPH: JAVIER ZAYAS PHOTOGRAPHY/GETTY IMAGES

WHEN GOOGLE CEO Sundar Pichai emailed his workers the company priorities for 2024 this month, developing AI responsibly was top of the list. Some employees now wonder whether Google can live up to that goal. The small team that has served as its primary internal AI ethics watchdog has lost its leader and is being restructured, according to four people familiar with the changes. A Google spokesperson says its work will continue in a stronger form going forward, but declined to provide details.

Google’s Responsible Innovation team, known as RESIN, was located inside the Office of Compliance and Integrity, in the company’s global affairs division. It reviewed internal projects for compatibility with Google’s AI principles that define rules for development and use of the technology, a crucial role as the company races to compete in generative AI. RESIN conducted over 500 reviews last year, including for the Bard chatbot, according to an annual report on AI principles work Google published this month.

RESIN’s role has looked uncertain since its leader and founder Jen Gennai, director of responsible innovation, suddenly left that role this month, say the sources, who spoke on the condition of anonymity to discuss personnel changes. Gennai’s LinkedIn profile lists her as an AI ethics and compliance adviser at Google as of this month, which sources say suggests she will soon leave based on how past departures from the company played out.

FEATURED VIDEO


Google Gemini Music Demo

Google split Gennai’s team of about 30 people into two, according to the sources. Company spokesperson Brian Gabriel says 10 percent of RESIN staffers will remain in place while 90% of the team were transferred to trust and safety, which fights abuse of Google services and also resides in the global affairs division. No one appears to have been laid off, sources say. The rationale for the changes and how responsibilities will be broken up couldn’t be learned. Some of the sources say they have not been told how AI principles reviews will be handled going forward.

Gabriel declined to say how RESIN’s work reviewing AI projects will be handled in the future but describes the shakeup as a signal of Google’s commitment to responsible AI development. The move “brought this particular Responsible AI team to the center of our well-established trust and safety efforts, which are baked into our product reviews and plans,” he says. “It will help us strengthen and scale our responsible innovation work across the company.”

Got a Tip?
Are you a current or former employee at Google? We’d like to hear from you. Using a nonwork phone or computer, contact Paresh Dave at paresh_dave@wired.com or on Signal/WhatsApp/Telegram at 1-415-565-1302.
Google is known for frequently reshuffling its ranks but RESIN had largely been untouched since the group’s founding. Though other teams, and hundreds of additional people, work on AI oversight at Google, RESIN was the most prominent, with a remit covering all Google’s core services.

In addition to the departure of its leader, Gennai, RESIN also saw one of its most influential members, Sara Tangdall, lead AI principles ethics specialist, leave this month. She is now responsible AI product director at Salesforce, according to her LinkedIn profile. Tangdall declined to comment and Gennai didn’t respond to calls for comment.

AI Uprising
Google created its Responsible Innovation team in 2018 not long after AI experts and others at the company publicly rose up in protest against a Pentagon contract called Project Maven that used Google algorithms to analyze drone surveillance imagery. RESIN became the core steward of a set of AI principles introduced after the protests, which say Google will use AI to benefit people, and never for weapons or undermining human rights. Gennai helped author the principles.

Teams from across Google could submit projects for review by RESIN, which provided feedback and sometimes blocked ideas seen as breaching the AI principles. The group stopped the release of AI image generators and voice synthesis algorithms that could be used to create deepfakes.

Seeking AI principles guidance is not mandatory for most teams, unlike reviews for privacy risks, which every project must undergo. But Gennai has said early reviews of AI systems pay off by preventing costly ethical breaches. “If implemented properly, Responsible AI makes products better by uncovering and working to reduce the harm that unfair bias can cause, improving transparency and increasing security,” she said during a Google conference in 2022.

That same year, RESIN moved into the global affairs division’s compliance unit. Google described the change in an annual report on AI principles work as ensuring “more centralized governance across all Google product areas,” but some team members feared it would tilt RESIN’s work more toward protecting Google than preventing harm to consumers.

As Google fights for positioning in a new AI boom and an era where some consumers are turning to TikTok or ChatGPT instead of Google Search, some employees now worry product development could become dangerously hasty. The restructuring of RESIN has increased those concerns, the sources say.

Google has spent the past year laying off thousands of workers and streamlining its operations to more quickly deliver advances to users and focus around a few AI initiatives. It has moved to shut down services, including its Podcasts app, and cut features from Google Assistant. The ad sales that fund its sprawling pursuits have grown less reliably in the post-pandemic economy and been trimmed by new regulations and court orders on privacy and anticompetitive behavior.

Pichai and other Google leaders have said they can accelerate AI development while still being responsible about its potential dangers. Google last year joined OpenAI, Microsoft, and several other big AI developers in joining a voluntary White House pledge to assess societal risks and national security concerns related to advanced AI.

RESIN is not the only group inside Google to have been disrupted as the company scrambles to compete in generative AI. Last year the company merged UK-based AI lab DeepMind into its primary research team, Google Brain, to unify development of the so-called foundational models that underpin tools such as Bard under a new entity called Google DeepMind.

As part of that change, ethical reviews for Google’s most advanced AI models, such as the recently released Gemini, fall not to RESIN but to Google DeepMind’s Responsibility and Safety Council, according to a technical paper published last month. RESIN has already left a mark on Google’s generative AI products, a company report says, such as by triggering a decision to limit Bard from using personal pronouns to try to avoid users treating it like a human. What role the company’s long-established AI watchdog will play on future developments is unclear.",
16,"Tech firms failing to ‘walk the walk’ on ethical AI, report says",https://www.aljazeera.com/news/2023/12/8/tech-firms-failing-to-walk-the-walk-on-ethical-ai-report-says,Rhea,News Article,,
17,An AI-generated image of a Victorian MP raises wider questions on digital ethics,https://www.abc.net.au/news/2024-02-01/georgie-purcell-ai-image-nine-news-apology-digital-ethics/103408440,Ned,News Article,"An AI-generated image of a Victorian MP raises wider questions on digital ethics
By Joseph Dunstan and Mikaela Ortolan
Topic:Artificial Intelligence

Thu 1 Feb
Thursday 1 February
A composite image shows Georgie Purcell in two photos which appear identical except for changes to her clothing.
Nine News apologised to Ms Purcell after broadcasting the digitally altered image of her on its nightly news (left). (Nine News / Twitter)

Link copied

Share article
It was an image broadcast for just a few moments on a Melbourne TV news bulletin, but it's since attracted international attention.

The digitally altered image of Victorian Animal Justice Party MP Georgie Purcell used to introduce a story on Victorian duck hunting saw the white dress she was wearing in the original photo swapped for a top exposing her midriff.

""Note the enlarged boobs and outfit to be made more revealing. Can't imagine this happening to a male MP,"" she tweeted.

After Ms Purcell called it out, media outlets including CNN and the BBC picked up the story, fuelling debate on the reach of generative artificial intelligence in our lives.

In its swift apology to Ms Purcell, broadcaster Nine News said the alteration had occurred due to an ""automation by Photoshop"" while resizing the original photo.

Ms Purcell says she's not sure she buys Nine's explanation but is happy to move on, provided a lesson is learnt by everyone to ensure it never happens again.

So how might the image have been made and what lessons should we take from it?

Expert says Nine's explanation is plausible
Nine News has told the ABC the alteration to Ms Purcell's midriff occurred when using Adobe Photoshop's ""generative expand"" tool.

The tool allows users to make an image bigger — the program uses AI to make assumptions or guesses about how that image might be best filled out with new material.

Nine said the image it used to produce the graphic was a more tightly cropped version of the Bendigo Advertiser's photo of Ms Purcell.

This image appears in some online image searches, and is cropped above Ms Purcell's waist.

Georgie Purcell stands in front of a river.
Nine News says this version of the photo, which is cropped higher than the original, was the version ingested into its system. (Bendigo Advertiser)

When that image was dragged downwards using the expand tool, the generated image exposing her midriff was created, according to Nine.

This was the image later incorporated into a graphic used in the network's coverage of debate over the future of duck hunting in Victoria.

Adobe released a statement saying edits to the image in question ""would have required human intervention and approval"".

A image of a TV news anchor and a graphic of duck hunting.
This graphic, including the altered image of Ms Purcell, appeared briefly before Nine News's TV report on duck hunting on Monday. (Nine News)

TJ Thomson, a senior lecturer in digital media at RMIT, said it was plausible that a cropped image could produce a range of different torsos when expanded using AI.

""If you are giving Photoshop less data about the body … it has to imagine a lot more about what's below the torso,"" he said.

""So it can have a lot more creative input into what is below the torso.""

To demonstrate the tool in question, we uploaded a photo of a consenting ABC employee to Photoshop, cut off just above the chin.

When the program was presented with the image and asked to perform the ""generative expand"" action, it produced several variants of the torso.

An AI-generated image of a man, with three alternative torsos. His face is blurred.
An image of a man created with AI generation below the neck, creating three alternative torsos.  (ABC News)

The clothing in each torso differed, including a buttoned-up shirt and one with several of the top buttons undone.

In one version, the program appeared to struggle to generate the hands, presenting a jumble of fingers at the end of the man's arms.

How a 'vast library' of images informs generative AI
Technology researcher Shaanan Cohney, who lectures at the University of Melbourne's Centre for AI and Digital Ethics, said Photoshop's generative fill tool was able to create new material for an image by drawing on a ""vast library of stock images"".

""Now, Adobe claims to have done this in a way that reduces the bias in the images out there,"" Dr Cohney said.

""But without more information, it's very hard for us to know what kinds of stereotypes might be in that information.""

Dr Thomson said we could infer that many generative AI tools trawled the web to draw on images from films and pop culture, creating results that reinforced the biases present in society.

""If we look at our own landscape in terms of cinema or stock photography, often we see very biased and under-representative images and movies,"" he said.

TJ Thomson standing in front of a blurry background with a large green plant.
TJ Thomson says AI was being used as a tool to spread misinformation. (Supplied: Anthony Weate)

""Quite white-dominated, quite middle-aged or younger, not a lot of old folks, not a lot of people with disabilities.

""And so all those biases then are reproduced or enhanced sometimes with AI, if you're not particularly prompting to try to get the more diverse representation.""

In a statement, an Adobe spokesperson said the company was committed to principles of accountability, responsibility and transparency when developing its AI products.

""Generative Fill and Generative Expand in Photoshop is powered by Adobe Firefly, Adobe's family of creative generative AI models, which is trained on the highest quality of assets including Adobe Stock images, openly license content and public domain content where copyright has expired,"" they said.

'Sexualised' images of women are training AI, researchers warn
When Ms Purcell saw her photoshopped body displayed on the TV, she said it was deeply upsetting.

""I don't think there's a single young person that hasn't struggled with their body image, and seeing your own body altered on TV is very confronting,"" she said.

""This has affected me in some way, and it could affect other women even more. It should never happen again.""

Walkley-award winning journalist Tracey Spicer said when she heard about Ms Purcell's experience she was ""horrified, but not surprised"".

Tracey Spicer speaks to ABC News
Tracey Spicer says media companies have an obligation to better train staff. (ABC News: Jerry Rickard)

Spicer, who recently authored a book examining the rise of artificial intelligence, said generative AI ""routinely sexualises images of women and girls"".

""In fact, while designing the cover for my book, we put in prompts to design an image of a strong robot woman looking to the future with hope but concern,"" Spicer said.

""[The AI generator] instead created an image of a sexy gold robot with huge breasts and a tiny waist.""

Spicer also backed a warning from the Victorian Women's Trust that the community was on ""the verge of a tsunami"" when it came to the weaponisation of AI against women.

Singer and songwriter Taylor Swift was the latest high-profile woman to be attacked using AI, after pornographic deepfake images were circulated online.

Taylor Swift, wearing a purple dress, poses for a photo.
A sexually explicit image of Taylor Swift that was AI generated was viewed 47 million times before the account was suspended. (Reuters: Steve Marcus)

Spicer said as technology became freer and easier to use, its potential to be abused was growing.

""Frankly, it's terrifying,"" she said.

""Many victim-survivors of this kind of abuse say it's like being physically, sexually assaulted.""

New York University data journalism researcher Meredith Broussard said the doctored image of Ms Purcell was an important reminder that AI ""keeps failing and screwing up"" in a range of ways and human intervention was critical.

A woman with curly hair and hoop earrings stands in front of a bookshelf smiling.
New York University data journalism researcher Meredith Broussard says we must interrogate AI's assumptions. (Supplied: Devin Curry)

""It's a lesson in why we shouldn't necessarily trust the defaults in artificial intelligence,"" she said.

""People think that AI is going to be somehow superior to humans and that's simply not the case.""

In Ms Purcell's case, Dr Broussard said the person operating the program that generated the altered image should have spotted the changes and opted not to use that image.

'Major harms' occurring due to racial biases
In the United States, Dr Broussard said there were several examples of ""major harms"" being carried out against people as a result.

""Facial recognition systems tend to be better at recognising light skin than dark skin, they tend to be better at recognising men than recognising women, they tend to not take trans and non-binary folks into account at all,"" she said.

""And so when facial recognition is used in policing, for example, it is disproportionately weaponised against communities of colour.""

A man looks at a demonstration on a big screen of software tracking the movement of people.
Consumer group CHOICE raised concerns some retailers were using facial recognition technology without the knowledge of customers. (Reuters: Thomas Peter)

Dr Thomson said it was also clear that AI was being used as a tool to spread misinformation.

""People are using these tools to make photorealistic generations that can deceive people or mislead people,"" he said.

""We're seeing that happening in the Hamas, Israel conflict and multiple other contexts,"" he said.

In Australia, retail giants Bunnings and Kmart are being investigated over their use of facial recognition technology in stores, amid privacy concerns.

Media companies urged to offer AI training
So where does this leave the media with its use of AI?

Dr Thomson said there were a range of different approaches being taken by outlets around the world.

Think you can spot content written by AI?
Photo shows Conceptual image of a robot office workerConceptual image of a robot office worker
You may not have heard of GPT-3, but there's a good chance you've read its work, used a website that runs its code, or even conversed with it through a chatbot or a character in a game.

""The majority of folks I've talked to are most comfortable using these tools for things like illustrations, things that are definitely not photorealistic, where you don't have the potential to mislead or deceive,"" he said.

""If you're doing a story on mental health or COVID, or that kind of thing, and you have some clearly illustrated image, that's AI-generated, that's the kind of test case or use case that photo editors feel most comfortable using.

""Some outlets are a bit more conservative, saying we're only going to use AI-generated images when we're reporting on AI-generated images that have gone viral, for example.""

At the ABC, there is a policy not to use generative AI images in place of real images in news reporting.

If one is used, it must be clearly labelled — as we've done in this story.

Spicer said media companies had an obligation to better train staff.

""Issues like this will only deepen the public's scepticism about journalism. That's the last thing we need, at a time when democracy is under threat,"" she said.

""There are things that journalists and media organisations can be doing right now. Staff should be trained in how to use AI safely and ethically.""

Calls for greater regulation in Australia
Several experts agree greater regulation of AI is needed — particularly in Australia.

In January, the Australian government unveiled plans to target some high-risk AI technologies with audits and safeguards — but Dr Thomson said more was needed to ""catch up"" with the rest of the world.

""Australia has been called kind of the back of the pack in terms of AI regulation,"" Dr Thomson said.

We asked an AI tool to 'paint' images of Australia. Critics say they're good enough to sell
Photo shows A painting of a couple embracing at a beachA painting of a couple embracing at a beach
Computer programs, trained on the internet and only invented this year have created these artworks based on a few text prompts.

In December, the European Union passed the world's first AI law, which paved the way for legal oversight of technology used in popular generative AI services.

In the United States, President Joe Biden made a major effort to regulate artificial intelligence under a new executive order where AI developers would need to risk-test their tools and share the results with the US government.

On Wednesday, Australian Securities and Investments Commission chair Joe Longo warned existing laws did not adequately prevent ""AI-facilitated harms"" before they occurred and further ""transparency and oversight"" may be needed.

Another idea flagged by the federal government is tagging AI-generated images with a watermark or in another way that ensures people seeing them are aware of how they've been made.

For its part, Adobe said that was an idea the company supported, likening it to health advice included on food packaging.

""We are working with customers and partners to enable content credentials — which are ""nutrition labels"" for digital content, that remain associated with content wherever it is used, published or stored,"" a spokesperson said.

A view over a person's shoulder as they look at a digitally altered image of a politician.
Researchers say better AI literacy across the community will become increasingly important as the technology expands. (ABC News: Gabriela Rahardja)

Dr Cohney said even with legal obligations, ""ethical norms"" and ""AI literacy"" in the community would be critical as the technology's use expanded.

Even if we have the perfect set of laws, they still won't catch every example of people not using them appropriately,"" he said.

""So we need to think about the way we train people, whether that be in the media, or whether they be working in another industry, to use AI in an ethical fashion.""",s
18,Why Ethical AI Must Be A Leadership Priority,https://www.forbes.com/sites/jonathanreichental/2024/05/22/why-ethical-ai-must-be-a-leadership-priority/,Ned,News Article,"Why Ethical AI Must Be A Leadership Priority
Jonathan Reichental, PhD
Contributor
I write about the role of technology in business and society.

Just because we can do something with artificial intelligence doesn't mean we should do it.GETTY
42 percent of companies are using artificial intelligence (AI) and an additional 40 percent are exploring uses, reports IBM. To underestimate AI value and delay adoption may be an existential risk for many organizations. Equally, embracing it too quickly without adequate attention paid to matters such as fairness, privacy, and accountability risks unpredictable negative consequences. Rather than an afterthought, the journey to broad AI adoption must include ethical AI: the implementation and management of AI solutions that prioritize responsible uses and reduced risks for all stakeholders.

Preparing For Revolutionary Change
The rapid progress of AI and its transformational impact on all aspects of business operations is nothing short of revolutionary. The remarkable speed at which AI is developing has all the characteristics of a change without historic parallel. Much like recent transformational technologies such as the Internet and smartphones, AI will join and perhaps outpace them as a new context for reinvention.

There’s a lot to be excited about, but poor moves early in any adoption have the potential to derail efforts or worse still, be a destructive force both to the organization and society. Producing output with absent or misguided rules, for example, can produce reputational risks and embarrassing results.

Specifically, generative AI relies heavily on vast quantities of historical data harvested from across the public Web and other accessible repositories. This data coupled with algorithms shapes what AI produces. If either one or both are mismanaged, the negative consequences are all too real.

Suddenly good intentions produce novel and unintended issues of fairness, discrimination, trust, privacy, transparency, and more. AI raises complex questions about its appropriate use with many still largely unanswered. To proactively mitigate unwanted outcomes including legal risks requires an ethical approach to AI.

The Role Of Ethical AI
The purpose of ethical AI is to support the responsible design and development of AI solutions that protect individuals, groups, and society, from harm. To succeed requires a combination of efforts that include computer science, policy implementation, and governance. Importantly, it must be a leadership priority.

Organizations will be best served by treating ethical AI not as a nice-to-have, but rather, an essential part of their AI efforts. Diligence can help protect the business, enable greater confidence in deployments, and assist with compliance with existing or future requirements. In fact, an increasing volume of national and international laws and regulations will make aspects of ethical AI required for organizations. Business leaders must become aware of their current obligations and be on top of developments as they emerge.

Steps Organizations Can Take Now
For organizations just beginning their AI journey, knowing where to begin with ethical AI can seem overwhelming. It can be perceived as an unwelcome effort that will slow or even disincentivize innovation. Despite these concerns, the benefits of ethical AI will eventually easily outweigh them.

It’s never too soon to begin implementing an approach to ethical AI and that includes those well into their AI developments and deployments. Here are three ways that all types of businesses can begin to increase their responsible use of AI.

Establish AI principles and standards: Identify what is most important to the organization, customers, products, and services, and use those to guide decisions.
Implement AI policies and procedures: Clearly outline how the organization will implement and enforce its principles and standards.
Create an AI governance capacity: Define how the organization will provide ongoing oversight for AI activities including monitoring efforts, providing training, and continuous improvement.
For too many, implementing ethical AI may not be a top-of-mind priority today, but it must become one. Leaders need to be convinced that doing the right thing with AI will get the right outcomes.

Creating and implementing a viable approach to ethical AI offers the best chance of avoiding significant issues. Without ethical AI, organizations may think they are moving in the fast lane only to wake up and find they are operating in the wrong lane.",
19,Your newsroom needs an AI ethics policy. Start here.,https://www.poynter.org/ethics-trust/2024/how-to-create-newsroom-artificial-intelligence-ethics-policy/,Ned,News Article,,
20,The Ethical Dilemma Of AI In Marketing: A Slippery Slope,https://forbes.com/sites/elijahclark/2024/03/14/the-ethical-dilemma-of-ai-in-marketing-a-slippery-slope/,Ned,News Article,,
21,Artificial intelligence and design: Questions of ethics,https://architectureau.com/articles/ai-and-design-questions-of-ethics/,Ned,Industry Article,"Artificial intelligence and design: Questions of ethics
As machines behave in increasingly “human-like” ways, questions of ethics arise in relation to the design and use of AI technologies. Architect and academic Nicole Gardner explains why it’s vital for designers to understand the fundamental principles of AI systems.

In the field of AI, ethical questions are concerned with how human (and non-human) actions, behaviours and choices affect the responsibilities we have to each other, to the environment and to future generations. In his research project at UNSW, associate lecturer Daniel Yu has used a convolutional neural network (CNN) to analyse satellite data of urban environments to predict ambient air temperatures.


In the field of AI, ethical questions are concerned with how human (and non-human) actions, behaviours and choices affect the responsibilities we have to each other, to the environment and to future generations. In his research project at UNSW, associate lecturer Daniel Yu has used a convolutional neural network (CNN) to analyse satellite data of urban environments to predict ambient air temperatures. 
Artificial intelligence (AI) is a very old idea, but the term AI and the field of AI as it relates to modern programmable digital computing have taken their contemporary forms in the past 70 years.1 Today, we interact with AI technologies constantly, as they power our web search engines, enable social media platforms to feed us targeted advertising, and drive our streaming service recommendations. Nonetheless, the release of OpenAI’s open-source AI chatbot ChatGPT in November 2022 signalled a game change because it demonstrated to the wider public a capacity for machines to behave in a “human-like” way. ChatGPT’s arrival also reignited questions and debates that have long preoccupied philosophers and ethicists. For example, if machines can behave like humans, what does this mean for our understanding of moral responsibility?

Across numerous professions, ethically charged questions are being asked about what tasks and responsibilities should be delegated to AI, what consequences might arise, and what or who is then responsible. These questions are concerned with how human (and non-human) actions, behaviours and choices affect the responsibilities we have to each other, to the environment and to future generations.

To discuss ethics in relation to AI, some definitions are required. Often, AI is misconstrued as a single thing, and narrowly conflated with machine learning (ML).2 Or, it is expansively associated with all kinds of algorithmic automation processes. When AI takes the form of a tool designed by humans to complete a task, AI is more correctly a system of technologies – including ML algorithms – that work together to simulate human intelligence or human cognitive functions such as seeing, conversing, under-standing and analysing. Further, AI systems are typically designed to act “with a significant degree of autonomy ” (emphasis mine) – a characteristic that is particularly important when it comes to ethical assessment.3

Ethical practice is about more than simply following rules. It involves examining and evaluating our choices in relation to their possible conse-quences, benefits and disbenefits.
Ethics is both an intellectual endeavour and an applied practice that can help us grapple with the choices and dilemmas we face in our daily work and life. It relates to AI in two ways: “AI ethics” focuses on the ethical dilemmas that arise in the design and use of AI technologies, while the “ethics of AI” encompasses the principles, codes, roadmaps, guides and toolkits that have been created to foster the design of ethical AI. While these provisions regarding the ethics of AI are useful, ethical practice is about more than simply following rules. It involves examining and evaluating our choices in relation to their possible consequences, benefits and disbenefits – in short, exercising moral imagination. As technology ethicist Cennydd Bowles puts it, ethics is quite simply a commitment to take our choices and even our lives seriously.”4

There are many kinds of AI technologies and systems, and their technical differences as well as contexts of use can bear significantly on the nature and scale of their associated ethical conundrums. For example, the ethical problem of “responsibility attribution” is amplified by generative AI systems that use deep learning5 because their inner workings (what happens between the layers of a neural network) can defy explanation. So, the capacity to truly reckon with the ethical significance of AI technologies relies on our ability to understand fundamental technical principles of AI systems in the context of both their design and use.

Since the 1960s, architects have investigated and debated the potential for AI applications in and for the design process. Today, AI technologies are becoming central to the development of industry-standard software. Autodesk is rapidly expanding its suite of AI tools, including by acquiring AI-powered design software companies such as Spacemaker (now Autodesk Forma). To understand and address the complex ethical dilemmas that arise in relation to the design and use of AI technologies in architecture, we can refer to guides and frameworks,6 but we can also apply our innate design thinking skills. We can ask “ethical questions” – not to land on a “yes” or a “no,” but to help us think through scenarios. This process gives us the oppor- tunity to uncover the less obvious or unintended consequences and externalities that the use of AI might bring into play.7

Let’s take an example: should designers use AI-powered automated space-planning tools in the design process? In this scenario, the stakeholders – those who stand to be impacted in some way – might include clients, designers, organizations, the profession and the environment. AI-driven space-planning tools could accelerate design processes and allow designers and clients to explore a broader range of layout options. Their use could also enable designers to spend more time on design evaluation, resulting in higher-quality design outputs. And the design firm might be able to reallocate saved time to upskill its employees in new competencies related to emerging technologies and digital literacy. But, the use of AI could also result in a redistribution of design labour and reduced employment. This ethical dilemma is common for most kinds of automation tools.

A dilemma more specific to AI-powered design tools concerns the ML training methods used to generate outputs. This connects to the responsible AI goal of “explainability” – the ability to comprehend how an AI system generates its outputs or makes decisions to identify impacts and potential hidden biases. For example, if an AI tool were trained on residential plans from another country, its spatial outputs might reflect country-specific cultural norms and regulatory requirements. This doesn’t mean that we should abandon the AI tool, but it does obligate us to keep numerous humans (designers) in the loop. Because even if, by regulation, a design technology company is required to disclose its training data and/or training methods, both design knowledge and technical expertise will be needed to understand what that means in practice.

Zooming out, we must also recognize that AI ethics extends beyond human entities and includes our obligations to non-humans – that is, the environment and its myriad life forms. As AI researcher Kate Crawford has shown, AI technologies rely on an enormous and extractive ecosystem, “from harvesting the data made from our daily activities and expressions, to depleting natural resources, and to exploiting labour around the globe.”8

AI technologies are morally significant because they are entangled in and mediate human decision-making in immeasurable ways. Architects, designers and educators must take responsibility for understanding how AI technologies operate– and what opportunities and pitfalls accompany their use in design practice. By enhancing digital literacy, and normalizing and scaffolding ethical reasoning skills in both the profession and in architecture and design education, we can see AI ethics not as a border guard but as an opportunity to “fertilise new ideas as well as weed out bad ones.”9",
22,This might be the most important job in AI,https://www.businessinsider.com/ai-chief-ethics-officer-2024-7,Ned,Industry Article,"This might be the most important job in AI
Lakshmi Varanasi Jul 21, 2024, 7:22 PM GMT+10

Share

Save
Name plate photo illustration.
There's an exciting new job in tech at the intersection of policy, ethics, and AI. Jenny Chang-Rodriguez/BI
A company's chief ethics officer ensures AI is used responsibly.
They define principles for regulating the tech, learn the legal landscape, and liaise with stakeholders.
Those in the role often earn annual salaries in the mid-six figures. 
Insider Today
Sign up to get the inside scoop on today’s biggest stories in markets, tech, and business — delivered daily. Read preview
Email address
Enter your email
Sign up
By clicking “Sign Up”, you accept our Terms of Service and Privacy Policy. You can opt-out at any time by visiting our Preferences page or by clicking ""unsubscribe"" at the bottom of the email.
Bull
The launch of ChatGPT ushered the corporate world into a new era.

The buzzy bot's technology — generative AI — could write emails, produce code, and materialize graphics in minutes. Suddenly, the days in which workers pored over their inboxes and painstakingly crafted presentations seemed like a relic of the past.

Companies, lured by profit and productivity gains, rushed to adopt the technology. According to a May survey from consulting firm Mckinsey & Company, 65% of the more than 1,300 companies it researched said they now regularly use generative AI — double the number using it the year before.

But the risks of misusing the technology loom large. Generative AI can hallucinate, spread misinformation, and reinforce biases against marginalized groups if it's not managed properly. Given that the technology relies on volumes of sensitive data, the potential for data breaches is also high. At worst, though, there's the danger that the more sophisticated it becomes, the less likely it is to align with human values.

With great power, then, comes great responsibility, and companies that make money from generative AI must also ensure they regulate it.

That's where a chief ethics officer comes in.

A critical role in the age of AI
The details of the role vary from company to company but — broadly — they're responsible for determining the impact a company's use of AI might have on the larger society, according to Var Shankar, the chief AI and privacy officer at Enzai, a software platform for AI governance, risk, and compliance. ""So beyond just your company and your bottom line, how does it affect your customers? How does it affect other people in the world? And then how does it affect the environment,"" he told Business Insider. Then comes ""building a program that standardizes and scales those questions every time you use AI.""

It's a role that gives policy nerds and philosophy majors, alongside programming whizzes, a footing in the fast-changing tech industry. And it often comes with a sizable annual paycheck in the mid-six figures.

Right now, though, companies aren't hiring people into these roles fast enough, according to Steve Mills, the chief AI ethics officer at Boston Consulting Group. ""I think there's a lot of talk about risk and principles, but little action to operationalize that within companies,"" he said.

A C-suite level responsibility
Those who are successful in the role ideally have four areas of expertise, according to Mills. They should have a technical grasp over generative AI, experience building and deploying products, an understanding of the major laws and regulations around AI, and significant experience hiring and making decisions at an organization.

Related stories

She interviewed for 15 AI roles before landing a Microsoft offer. It made her realize how the job market has changed.


'Botshit' is an example of how AI is making customer service worse

""Too often, I see people put midlevel managers in charge, and while they may have expertise, desire, and passion, they typically don't have the stature to change things within the organization and rally legal, business, and compliance teams together,"" he said. Every Fortune 500 company using AI at scale needs to charge an executive with overseeing a responsible AI program, he added.

Shankar, a lawyer by training, said that the role doesn't warrant any specific educational background. The most important qualification is understanding a company's data. That means having a handle on the ""ethical implications of the data that you collect, use, where it comes from, where it was before it was in your organization, and what kinds of consent you have around it,"" he said.

He pointed to the example of healthcare providers who could unintentionally perpetuate biases if they don't have a firm grasp of their data. In a study published in Science, hospitals and health insurance companies that used an algorithm to identify patients that would benefit from ""high-risk care management"" ended up prioritizing healthier white patients over sicker black patients. That's the kind of blunder an ethics officer can help companies avoid.

Collaborating across companies and industries
Those in the role should also be able to communicate confidently with various stakeholders.

Christina Montgomery, IBM's vice president, chief privacy and trust officer, and chair of its AI Ethics Board, told BI that her days are usually packed with client meetings and events, alongside other responsibilities.

""I spent a lot of time externally, probably more time lately, in speaking at events and engaging with policymakers and on the external boards because I feel like we have very much an opportunity to influence and determine what the future looks like,"" she said.

She sits on boards like the International Association of Privacy Professionals, which recently launched an Artificial Intelligence Governance Professional certification for individuals who want to lead the field of AI ethics. She also engages with government leaders and other chief ethics officers.

""I think it's absolutely critical that we be talking to each other on a regular basis and sharing best practices, and we do a lot of that across companies,"" she said.

She aims to develop a broader understanding of what's happening on a societal level — something she sees as key to the role.

""My fear at the space that we are at right now is that there's no interoperability globally among all these regulations, and what's expected, and what's right and wrong in terms of what companies are going to have to comply with,"" she said. ""We can't operate in a world that way. So the conversations among companies, governments, and boards are so important right now.""",
23,Are tomorrow’s engineers ready to face AI’s ethical challenges?,https://theconversation.com/are-tomorrows-engineers-ready-to-face-ais-ethical-challenges-213826,Ned,Research Paper,"Are tomorrow’s engineers ready to face AI’s ethical challenges?
A chatbot turns hostile. A test version of a Roomba vacuum collects images of users in private situations. A Black woman is falsely identified as a suspect on the basis of facial recognition software, which tends to be less accurate at identifying women and people of color.

These incidents are not just glitches, but examples of more fundamental problems. As artificial intelligence and machine learning tools become more integrated into daily life, ethical considerations are growing, from privacy issues and race and gender biases in coding to the spread of misinformation.

The general public depends on software engineers and computer scientists to ensure these technologies are created in a safe and ethical manner. As a sociologist and doctoral candidate interested in science, technology, engineering and math education, we are currently researching how engineers in many different fields learn and understand their responsibilities to the public.

Yet our recent research, as well as that of other scholars, points to a troubling reality: The next generation of engineers often seem unprepared to grapple with the social implications of their work. What’s more, some appear apathetic about the moral dilemmas their careers may bring – just as advances in AI intensify such dilemmas.

Make better decisions - find out what the experts think.
Aware, but unprepared
As part of our ongoing research, we interviewed more than 60 electrical engineering and computer science masters students at a top engineering program in the United States. We asked students about their experiences with ethical challenges in engineering, their knowledge of ethical dilemmas in the field and how they would respond to scenarios in the future.

First, the good news: Most students recognized potential dangers of AI and expressed concern about personal privacy and the potential to cause harm – like how race and gender biases can be written into algorithms, intentionally or unintentionally.

One student, for example, expressed dismay at the environmental impact of AI, saying AI companies are using “more and more greenhouse power, [for] minimal benefits.” Others discussed concerns about where and how AIs are being applied, including for military technology and to generate falsified information and images.

When asked, however, “Do you feel equipped to respond in concerning or unethical situations?” students often said no.

“Flat out no. … It is kind of scary,” one student replied. “Do YOU know who I’m supposed to go to?”

Another was troubled by the lack of training: “I [would be] dealing with that with no experience. … Who knows how I’ll react.”

Two young women, one Black and one Asian, sit at a table together as they work on two laptops.
Many students are worried about ethics in their field – but that doesn’t mean they feel prepared to deal with the challenges. The Good Brigade/DigitalVision via Getty Images
Other researchers have similarly found that many engineering students do not feel satisfied with the ethics training they do receive. Common training usually emphasizes professional codes of conduct, rather than the complex socio-technical factors underlying ethical decision-making. Research suggests that even when presented with particular scenarios or case studies, engineering students often struggle to recognize ethical dilemmas.

‘A box to check off’
Accredited engineering programs are required to “include topics related to professional and ethical responsibilities” in some capacity.

Yet ethics training is rarely emphasized in the formal curricula. A study assessing undergraduate STEM curricula in the U.S. found that coverage of ethical issues varied greatly in terms of content, amount and how seriously it is presented. Additionally, an analysis of academic literature about engineering education found that ethics is often considered nonessential training.

Many engineering faculty express dissatisfaction with students’ understanding, but report feeling pressure from engineering colleagues and students themselves to prioritize technical skills in their limited class time.

Researchers in one 2018 study interviewed over 50 engineering faculty and documented hesitancy – and sometimes even outright resistance – toward incorporating public welfare issues into their engineering classes. More than a quarter of professors they interviewed saw ethics and societal impacts as outside “real” engineering work.

About a third of students we interviewed in our ongoing research project share this seeming apathy toward ethics training, referring to ethics classes as “just a box to check off.”

“If I’m paying money to attend ethics class as an engineer, I’m going to be furious,” one said.

These attitudes sometimes extend to how students view engineers’ role in society. One interviewee in our current study, for example, said that an engineer’s “responsibility is just to create that thing, design that thing and … tell people how to use it. [Misusage] issues are not their concern.”

One of us, Erin Cech, followed a cohort of 326 engineering students from four U.S. colleges. This research, published in 2014, suggested that engineers actually became less concerned over the course of their degree about their ethical responsibilities and understanding the public consequences of technology. Following them after they left college, we found that their concerns regarding ethics did not rebound once these new graduates entered the workforce.

Joining the work world
When engineers do receive ethics training as part of their degree, it seems to work.

Along with engineering professor Cynthia Finelli, we conducted a survey of over 500 employed engineers. Engineers who received formal ethics and public welfare training in school are more likely to understand their responsibility to the public in their professional roles, and recognize the need for collective problem solving. Compared to engineers who did not receive training, they were 30% more likely to have noticed an ethical issue in their workplace and 52% more likely to have taken action.

An Asian man wearing glasses stares seriously into space, standing against a holographic background in shades of pink and blue.
The next generation needs to be prepared for ethical questions, not just technical ones. Qi Yang/Moment via Getty Images
Over a quarter of these practicing engineers reported encountering a concerning ethical situation at work. Yet approximately one-third said they have never received training in public welfare – not during their education, and not during their career.

This gap in ethics education raises serious questions about how well-prepared the next generation of engineers will be to navigate the complex ethical landscape of their field, especially when it comes to AI.

To be sure, the burden of watching out for public welfare is not shouldered by engineers, designers and programmers alone. Companies and legislators share the responsibility.

But the people who are designing, testing and fine-tuning this technology are the public’s first line of defense. We believe educational programs owe it to them – and the rest of us – to take this training seriously.",
24,Corporate directors weigh AI ethics at first-of-its-kind forum,https://news.harvard.edu/gazette/story/newsplus/corporate-directors-weigh-ai-ethics-at-first-of-its-kind-forum/,Awais,News Article,,
25,Why Pope Francis thinks the Church should play a part in world leaders’ debate on AI,https://edition.cnn.com/2024/06/12/tech/pope-francis-g7-artificial-intelligence-intl-cmd/index.html,Dan,News Article,,
26,Generative AI in business: how to navigate the ethics,https://intheblack.cpaaustralia.com.au/ethics-and-governance/generative-ai-in-business-navigate-ethics,Ned,Industry Article,,
27,The ethics of artificial intelligence and machine learning,https://thethaiger.com/hot-news/technology/the-ethics-of-artificial-intelligence-and-machine-learning,Dan,News Article,,
28,Readiness Assessment Methodology,https://www.unesco.org/ethics-ai/en/ram,Awais,Industry Article,,
29,ARTIFICIAL INTELLIGENCE Ethical Issues Regarding the Use of Artificial Intelligence,https://auanews.net/issues/articles/2024/february-extra-2024/artificial-intelligence-ethical-issues-regarding-the-use-of-artificial-intelligence,Hiruni,Research Paper,,
30,How to Implement AI — Responsibly,https://hbr.org/2024/05/how-to-implement-ai-responsibly,Rhea,Research Paper,,
31,A Perspective from the UNESCO AI Ethics Business Council at the Eastern Africa sub-Regional Forum on AI,https://www.unesco.org/en/articles/perspective-unesco-ai-ethics-business-council-eastern-africa-sub-regional-forum-ai,Awais,Industry Article,,
32,Addressing equity and ethics in artificial intelligence,https://www.apa.org/monitor/2024/04/addressing-equity-ethics-artificial-intelligence,Awais,Industry Article,,
33,Ethical AI: Ensuring Responsible and Fair Artificial Intelligence,https://www.hp.com/au-en/shop/tech-takes/post/ethical-ai-use,Awais,Industry Article,,
34,Responsible AI is a competitive advantage,https://www.ibm.com/blog/responsible-ai-is-a-competitive-advantage/,Awais,Industry Article,,
35,Pope asks world's religions to push for ethical AI development,https://www.usccb.org/news/2024/pope-asks-worlds-religions-push-ethical-ai-development,Awais,Industry Article,,
36,AI legislations ‘behind’ where the technology is,https://www.news.com.au/finance/ai-legislations-behind-where-the-technology-is/video/5abbe947cdfa4b4793938f433d5bef3c,Ned,News Article,,
37,Pope Francis to weigh in on 'ethical' AI at G7 summit,https://www.news.com.au/breaking-news/pope-francis-to-weigh-in-on-ethical-ai-at-g7-summit/news-story/3833539f0a157776b06d2393ae433dfa,Ned,News Article,,
38,AI Ethics: 8 global tech companies commit to apply UNESCO’s Recommendation,https://www.unesco.org/en/articles/ai-ethics-8-global-tech-companies-commit-apply-unescos-recommendation,Awais,Industry Article,,
39,AI ethical review should empower innovation—not prevent it,https://www.fastcompany.com/91069648/ai-ethical-review-should-empower-innovation-not-prevent-it,Rhea,Industry Article,,
40,"Good AI, bad AI: decoding responsible artificial intelligence",https://www.csiro.au/en/news/All/Articles/2023/November/Responsible-AI-explainer,Awais,Industry Article,,
41,Significant gap in ethical AI implementation: Accenture,https://www.digitalnationaus.com.au/news/significant-gap-in-ethical-ai-implementation-accenture-609900,Dan,Industry Article,,
42,AI Isn’t Ready to Make Unsupervised Decisions,https://hbr.org/2022/09/ai-isnt-ready-to-make-unsupervised-decisions,Hiruni,News Article,,
43,Leading AI Ethics: UNESCO and KOMINFO launch AI Readiness Assessment Methodology in Indonesia,https://www.unesco.org/en/articles/leading-ai-ethics-unesco-and-kominfo-launch-ai-readiness-assessment-methodology-indonesia,Awais,Industry Article,,
44,Exploring the ethics of Artificial Intelligence in art,https://www.artshub.com.au/news/opinions-analysis/exploring-the-ethics-of-artificial-intelligence-in-art-2694121/,Dan,News Article,,
45,Dell teams up with governments worldwide to tackle issues of AI ethics,/www.business-standard.com/companies/news/dell-teams-up-with-governments-worldwide-to-tackle-issues-of-ai-ethics-124052300216_1.html,Rhea,News Article,,
46,The Ethics Of AI: Balancing Innovation With Responsibility,https://www.forbes.com/sites/forbestechcouncil/2024/02/08/the-ethics-of-ai-balancing-innovation-with-responsibility/,Lida,News Article,,
47,"In the rush to adopt AI, ethics and responsibility are taking a backseat at many companies",https://www.businessinsider.com/ai-ethics-risks-responsibility-business-bcg-2024-5,Dan,News Article,,
48,A rapid review of responsible ai frameworks: How to guide the development of ethical ai,https://ieeexplore.ieee.org/ielx7/8566059/8995808/09001063.pdf?tp=&arnumber=9001063&isnumber=8995808&ref=aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uYXUv,Lida,Research Paper,,
49,Ordinary Ethics of Governing AI,https://carnegieendowment.org/2024/04/30/ordinary-ethics-of-governing-ai-pub-92329,Awais,Industry Article,,
50,Principles to Practices for Responsible AI: Closing the Gap,https://arxiv.org/abs/2006.04707,Lida,Research Paper,,
51,The ethics of AI image making,https://www.australiangeographic.com.au/topics/opinion-and-analysis/2023/11/the-ethics-of-ai-photography/,Awais,News Article,,
52,AI Generates Debate Over Newsroom Ethics,https://www.voanews.com/a/ai-generates-debate-over-newsroom-ethics-/7450743.html,Awais,News Article,,
53,Artificial intelligence and ethics: best practices for companies,https://www.orange.com/en/magazines/can-we-move-towards-responsible-ai/artificial-intelligence-and-ethics-best-practices,Awais,Industry Article,,
54,The ethics of advanced AI assistants,https://deepmind.google/discover/blog/the-ethics-of-advanced-ai-assistants/,Awais,Industry Article,,
55,"AI ethics are ignoring children, say Oxford researchers",https://www.ox.ac.uk/news/2024-03-21-ai-ethics-are-ignoring-children-say-oxford-researchers,Awais,Industry Article,,
56,"Fairness, Transparency, and Human Involvement: The Ethical Side of Artificial Intelligence",https://news.sap.com/2024/02/ethical-side-of-artificial-intelligence/,Awais,Industry Article,,
57,Why we need cybersecurity of AI: ethics and responsible innovation,https://www.weforum.org/agenda/2023/12/cybersecurity-ai-ethics-responsible-innovation/,Awais,Industry Article,,
58,For the planet and people: IBM’s focus on AI ethics in sustainability,https://www.ibm.com/blog/for-the-planet-and-people-ibms-focus-on-ai-ethics-in-sustainability/,Awais,Industry Article,,
59,2 in 5 customers are concerned about AI ethics,https://www.marketingtechnews.net/news/2024/may/30/2-in-5-customers-are-concerned-about-ai-ethics/,Awais,News Article,,
60,A look into IBM’s AI ethics governance framework,https://www.ibm.com/blog/a-look-into-ibms-ai-ethics-governance-framework/,Awais,Industry Article,,
61,Davos 2024: Can – and should – leaders aim to regulate AI directly?,https://www.bbc.com/worklife/article/20240118-davos-2024-can-and-should-leaders-aim-to-regulate-ai-directly,Hiruni,News Article,,
62,Responsible AI—two frameworks for ethical design practice,https://ieeexplore.ieee.org/ielx7/8566059/8995808/09001063.pdf?tp=&arnumber=9001063&isnumber=8995808&ref=aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uYXUv,Lida,Research Paper,,
63,A physicists’ guide to the ethics of artificial intelligence,https://www.symmetrymagazine.org/article/a-physicists-guide-to-the-ethics-of-artificial-intelligence?language_content_entity=und,Awais,News Article,,
64,Expanding on ethical considerations of foundation models,https://www.ibm.com/blog/expanding-on-ethical-considerations-of-foundation-models/,Awais,Industry Article,,
65,Of Ethics and Values: A Reflection on the AI versus Artists Discourse,https://moderndiplomacy.eu/2024/07/21/of-ethics-and-values-a-reflection-on-the-ai-versus-artists-discourse/,Rhea,Research Paper,,
66,Tech sector examines the risks and rewards of AI. How two paths converged at Northeastern’s London campus,https://news.northeastern.edu/2024/07/17/women-in-ai-ethics/,Rhea,Research Paper,,
67,AI Ethics: 7 Crucial Qualities Of Ethical Leadership,https://www.forbes.com/sites/bruceweinstein/2024/02/21/ai-7-crucial-qualities-of-ethical-leadership/,Awais,News Article,,
68,Our commitment to ethical and responsible use of AI,https://www.novartis.com/esg/ethics-risk-and-compliance/compliance/our-commitment-ethical-and-responsible-use-ai,Awais,Industry Article,,
69,"Connecting the dots in trustworthy Artificial Intelligence: From AI principles, ethics, and key requirements to responsible AI systems and regulation",https://www.sciencedirect.com/science/article/pii/S1566253523002129,Lida,Research Paper,,
70,AI could transform ethics committees,https://theconversation.com/ai-could-transform-ethics-committees-224424,Rhea,Research Paper,,
71,"How Ethics, Regulations And Guidelines Can Shape Responsible AI",https://www.forbes.com/sites/forbestechcouncil/2024/02/05/how-ethics-regulations-and-guidelines-can-shape-responsible-ai/,Awais,News Article,,
72,From Our Fellows –  From Automation to Agency: The Future of AI Ethics Education,https://cdt.org/insights/from-automation-to-agency-the-future-of-ai-ethics-education/,Awais,Industry Article,,
73,"‘Uncovered, unknown, and uncertain’: Guiding ethics in the age of AI",https://news.yale.edu/2024/02/21/uncovered-unknown-and-uncertain-guiding-ethics-age-ai,Awais,News Article,,
74,How Do You Ensure AI Ethics In Insurtech? ,https://www.corelogic.com/intelligence/ai-ethics-insurance-industry/,Awais,Industry Article,,
75,A Maturity Model for Industries and Organizations of All Types to Adopt Responsible AI—Preliminary Results,https://dl.acm.org/doi/abs/10.1007/978-3-031-49008-8_6,Lida,Research Paper,,
76,Cisco joins Rome Call as AI ethics debate widens,https://newsroom.cisco.com/c/r/newsroom/en/us/a/y2024/m04/cisco-joins-rome-call-as-ai-ethics-debate-widens.html,Awais,Industry Article,,
77,How to reduce the ethical dangers of AI-assisted farming,https://aeon.co/essays/how-to-reduce-the-ethical-dangers-of-ai-assisted-farming,Awais,Industry Article,,
78,Yale Freshman Creates AI Chatbot With Answers on AI Ethics,https://www.insidehighered.com/news/tech-innovation/artificial-intelligence/2024/05/02/yale-freshman-creates-ai-chatbot-answers-ai,Awais,News Article,,
79,"Tackling Bias, Inequality, Lack of Privacy – New WHO Guidelines on AI Ethics and Governance are Released",https://healthpolicy-watch.news/tackling-bias-inequality-lack-of-privacy-new-who-guidelines-on-ai-ethics-and-governance-are-released/,Awais,Industry Article,,
80,NIST Researchers Suggest Historical Precedent for Ethical AI Research,https://www.nist.gov/news-events/news/2024/02/nist-researchers-suggest-historical-precedent-ethical-ai-research,Awais,Industry Article,,
81,Oxford Institute for Ethics in AI to host ground-breaking AI Ethics Conference,https://www.ox.ac.uk/news/2024-06-19-oxford-institute-ethics-ai-host-ground-breaking-ai-ethics-conference,Awais,Industry Article,,
82,Tech Mahindra: The Rise of the AI Ethics Engineer,https://cybermagazine.com/articles/tech-mahindras-new-role-the-ai-ethics-engineer,Awais,Industry Article,,
83,UNESCO wants to assist Bangladesh in ethical AI development,https://coingeek.com/unesco-wants-to-assist-bangladesh-in-ethical-ai-development/,Awais,Industry Article,,
84,"When AI, Ethics and Data Privacy Collide, What Comes Next?",https://www.adp.com/spark/articles/2024/02/when-ai-ethics-and-data-privacy-collide-what-comes-next.aspx,Awais,Industry Article,,
85,"In rush to implement AI, ethics take a backseat at many companies",https://www.businessinsider.com/ai-ethics-risks-responsibility-business-bcg-2024-5,Awais,News Article,,
86,Ethical challenges in AI-enhanced military operations,https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2023.1229252/full,Awais,Research Paper,,
87,Adopting and expanding ethical principles for generative artificial intelligence from military to healthcare,https://www.nature.com/articles/s41746-023-00965-x,Awais,Research Paper,,
88,"AI: Tackling Ethics, Equality and Misinformation at Davos",https://sustainabilitymag.com/tech-ai/tackling-ai-ethics-equality-and-misinformation-at-davos,Awais,News Article,,
89,AI will be everywhere. How should we prepare for the ethics around this?,https://www.thenationalnews.com/opinion/comment/2024/05/17/ai-ethics-google-generative/,Dan,News Article,,
90,Address ethical concerns to optimise AI use,https://www.tribuneindia.com/news/comment/address-ethical-concerns-to-optimise-ai-use-640753,Awais,News Article,,
91,"Meta, Apple, Microsoft Expand AI Risk Transparency Amid Pressure",https://news.bloomberglaw.com/esg/meta-apple-microsoft-move-to-fend-off-mounting-ai-concerns,Awais,News Article,,
93,Original sins and dirty secrets: GenAI has an ethics problem. These are the three things it most urgently needs to fix,https://fortune.com/2024/06/27/gen-ai-ethics-original-sins-data-theft-labor-exploitation-environmental-impact/,Dan,News Article,,
94,Column: The ethics of AI,https://www.dailyeasternnews.com/2024/05/28/column-the-ethics-of-ai/,Dan,News Article,,
95,"Philosophy, ethics, and the pursuit of 'responsible' artificial intelligence",https://www.rit.edu/news/philosophy-ethics-and-pursuit-responsible-artificial-intelligence,Dan,News Article,,
96,The Ethics of AI: How Far Is Too Far?,https://www.barna.com/research/ai-ethics/,Rhea,News Article,,
97,How Taylor Swift’s Experience With ‘Deepfakes’ Can Help Students Examine AI Ethics,https://www.edweek.org/technology/how-taylor-swifts-experience-with-deepfakes-can-help-students-examine-ai-ethics/2024/01,Dan,News Article,,
98,"Friar Tech: The Vatican's top AI ethics expert who advises Pope Francis, the UN and Silicon Valley",https://www.euronews.com/next/2024/01/21/friar-tech-the-vaticans-top-ai-ethics-expert-who-advises-pope-francis-the-un-and-silicon-v,Awais,News Article,,
99,"The top 12 people in AI policy, ethics, and research",https://www.businessinsider.com/ai-100-top-12-people-policy-ethics-and-research-2023-11,Awais,News Article,,
100,"Apple Investors Reject Bid for Transparency on AI Risks, Ethics",https://news.bloomberglaw.com/esg/apple-investors-reject-bid-for-transparency-on-ai-risks-ethics,Awais,News Article,,
101,Scarlett Johansson Speaks on Disney ‘Black Widow’ Dispute and AI Ethics with OpenAi: ‘I Don’t Hold a Grudge’,https://whatstrending.com/scarlett-johansson-speaks-on-disney-black-widow-dispute-and-ai-ethics-with-openai-i-dont-hold-a-grudge/,Awais,News Article,,
102,Study: Humans Rate Artificial Intelligence as More ‘Moral’ Than Other People,https://news.gsu.edu/2024/05/06/study-humans-rate-artificial-intelligence-as-more-moral-than-other-people/,Awais,News Article,,