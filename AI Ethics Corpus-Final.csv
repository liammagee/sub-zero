#,Title,Article Link,Contributor,Type,Notes,Leftie Comment,Rightie Comment
9,How businesses can responsibly use AI and address ethical and security challenges,https://legal.thomsonreuters.com/blog/how-to-responsibly-use-ai-to-address-ethical-and-risk-challenges/,Ned,News Article,"How businesses can responsibly use AI and address ethical and security challenges


Addressing ethical and security concerns in AI usage, emphasizing transparency, accountability, and responsible governance frameworks.

In our Future of Professionals report, we address the many concerns and challenges that are being raised around the use and application of AI in professional and business capacities. Legal and accounting businesses are faced with figuring out how to use AI ethically and responsibly. Thomson Reuters has been a thought leader in developing principles to guide practices and helping our clients navigate the complexity with AI for decades. As generative artificial intelligence continues to infiltrate our lives personally and professionally, we strive to provide the most up-to-date research and best practices on leveraging these technologies with accountability and transparency. 

Ethical and security issues of AI
While many legal and accounting firms are trying to determine their AI practices and governance, there are a number of issues that need to be addressed. Older generations are being challenged to adapt to the changes the incorporation of AI into professional services is having and may offer resistance from a fear that AI will push ethics out the window. While younger generations are looking at the possibilities generative AI opens. However, there seems to be little debate over the fact that businesses that embrace AI will be the ones to get ahead. 

Across many industries, 15% of professionals reported data security and ethics as their biggest fear, with a lack of transparency and accountability close behind. The appeal of AI is that it can assist with numerous repetitive, time-consuming activities and tasks within the operations of a professional firm or department. It has also been found to improve mental health by relieving anxiety, isolation, and burnout by creating more tool-based capacity for this type of work increasing the amount of time professionals have to nurture client relationships and grow their client base.  


While there are other fears around accuracy and loss of jobs, data security and ethics make a combined 30%.
As much as AI can help businesses and departments do beneficial work, there are ethical considerations to take into account. AI can help fraudsters conduct their activities more efficiently and accurately. It is also important to always remember that AI generates responses based on algorithms created by humans and information provided by humans. As such, humans should be held accountable for verifying and fact-checking what AI generates.  

The eagerness to leverage AI has created some situations that have highlighted the necessity of consent and accountability, including cases where AI has “hallucinated” an answer – meaning it improvised a response based on the information it had – which led to legal and ethical consequences. Since AI does not provide any data on the source of the responses, ensuring a human validation process should be a part of AI policies. AI is also subject to bias risk mitigation, in which the data used to train a model may result in errors favoring one outcome over another. This can result in a lack of trust and fairness.  

 

Future of Professionals Report: How AI is the catalyst for transforming every aspect of work        
Future of Professionals Report
How AI is the Catalyst for Transforming Every Aspect of Work

View report
 

 

Using AI responsibly
The United States has issued an Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence that establishes principles and guidelines for federal agencies to follow when implementing an AI system. It also offers a framework for working with various stakeholders. The Future of Professionals report shows that 52% of professionals believe that regulations governing the professional ethics of AI are a necessity, and 25% believe that governments should be designing and overseeing professional ethics regulations. 


Overall, over 75% of professionals heavily believe the profession and the government should regulate AI.
Key areas of AI governance frameworks should cover encryption and authentication protocols, frequent auditing and testing procedures, traceability, educating employees on the proper and ethical use of AI, and best practices for securing and protecting confidential data. Ultimately, businesses and departments should be able to understand how an algorithm arrives at its output and where the data comes from. Because it pulls data from large data sets, those data sets should be legitimate and qualified to get the most accurate and relevant output.  

If AI is to be incorporated into your workflow, a human-centric design should be at the core of the initiative. Transparency and accountability are all crucial to maintaining trust with clients, users, and employees. Biases should be avoided, fairness should be promoted, and security should be the top priority.  

Given the private and confidential nature of the information that professionals are working with, we believe that developing internal AI regulations and governance at the firm level is critical to establishing trust, accountability, and transparency around the consent and privacy of client information. It’s important to understand the principles behind AI ethics and make sure your organization has a framework that is tailored to the business and your customers. 

Confidence in your adoption of AI for all stakeholders lies in professionals’ ability to provide high levels of service, access to information on the use of AI, and foster a culture of risk mitigation and awareness. For more information, download the Future of Professionals report.  ",,
14,6 Critical – And Urgent – Ethics Issues With AI,https://www.forbes.com/sites/eliamdur/2024/01/24/6-critical--and-urgent--ethics-issues-with-ai/,Rhea,News Article,"6 Critical – And Urgent – Ethics Issues With AI
Eli Amdur
Contributor
Leadership professor, job market journalist-analyst, business advisor.
Jan 24, 2024,11:50am EST
Code of Ethics in Technology as a Business ConceptGETTY
There is no doubt in most people’s minds that AI is one of the most transformative technologies of our time. There is no doubt in mine that it is the most transformative technology ever.

The “of our time” position has been borne out already. By the end of 2024, my “ever” position will likely be validated as well.

AI has lost no time in unfolding its immense potential to improve efficiency, aid research and discovery, enhance decision-making, and solve complex problems – all at a blinding rate of acceleration. It is precisely this rate of acceleration that’s propelling the future towards us faster than ever before, the reason I believe that will lead us to the conclusion this year that AI will have surpassed all other advances ever made.

The cold, hard truth
Given AI’s potential to be a more powerful positive force than any other in history, it stands to reason that it will have the same possibility on the negative side of the equation, as it’s been with every other invention or discovery ever. This also raises significant ethical concerns that demand our attention and thoughtful consideration.

So I did an informal poll of six experts with lots of AI experience – as practitioners and as academics – and asked them, simply, what we should be worried about. How, I asked them, should we and could we ensure that AI systems act ethically? On six issues, here’s their collective thinking.

MORE FROMFORBES ADVISOR
Best High-Yield Savings Accounts Of September 2023
ByKevin PayneContributor
Best 5% Interest Savings Accounts of September 2023
ByCassidy HortonContributor
1. Data Bias
One of the foremost ethical concerns surrounding AI is data bias. AI systems are only as good as the data they’re trained on, so objective data curation becomes paramount. Thus, developers and researchers must prioritize and standardize rigorous testing and continuous monitoring.

2. Privacy
As AI systems become more sophisticated and far reaching in data collection and analysis, the line between security and surveillance blurs. From facial recognition to smart home devices, the potential for invasions of privacy, not to mention election tampering and corporate hacking, is ominous.

3. Accountability
As AI systems make more decisions that impact our lives, it becomes more critical to establish clear lines of responsibility. Who should be held accountable when an autonomous vehicle makes a mistake? Or when healthcare diagnoses are made and decisions about medications or therapies are carried out? This applies in the legal arena, too.

4. Job Displacement
New technologies inevitably lead to job losses in old industries but even greater job creation in new ones. Seamless transition from old to new industries depends on a four-part coalition among: the individuals who need the jobs, the employers who will offer them, higher education that will develop a skilled workforce, and government that will fund this. At the same time, must be remembered that, over the last three years, progress has been made to shrink the income and wealth gap, ground tht has been gained and that cannot be relinquished. This is nothing less than an issue of national interest.

5. Transparency
All AI stakeholders – producers, educators, users, and casual observers – deserve to have a clear understanding of how AI systems make decisions. Algorithms are just as easily sinister as they are life supporting, making scrutiny a key factor.

6. What’s Ahead?
We’re nowhere near the development of superintelligent AI – the 800-pound gorilla in the room – but we’re closer than we think, as the rate of acceleration comes into play. As we move closer to creating AI systems that surpass human intelligence, questions about their control and alignment with human values come to the fore. If you haven’t seen the film 2001: A Space Odyssey yet, do not wait any longer. Safeguards must be in place to prevent AI from evolving in ways that could threaten humanity. We can start by defaulting to Isaac Asimov’s Three Laws of Robotics, a good place for me to end this essay and implore you to look it up and continue from there.",,
17,An AI-generated image of a Victorian MP raises wider questions on digital ethics,https://www.abc.net.au/news/2024-02-01/georgie-purcell-ai-image-nine-news-apology-digital-ethics/103408440,Ned,News Article,"An AI-generated image of a Victorian MP raises wider questions on digital ethics
By Joseph Dunstan and Mikaela Ortolan
Topic:Artificial Intelligence

Thu 1 Feb
Thursday 1 February
A composite image shows Georgie Purcell in two photos which appear identical except for changes to her clothing.
Nine News apologised to Ms Purcell after broadcasting the digitally altered image of her on its nightly news (left). (Nine News / Twitter)

Link copied

Share article
It was an image broadcast for just a few moments on a Melbourne TV news bulletin, but it's since attracted international attention.

The digitally altered image of Victorian Animal Justice Party MP Georgie Purcell used to introduce a story on Victorian duck hunting saw the white dress she was wearing in the original photo swapped for a top exposing her midriff.

""Note the enlarged boobs and outfit to be made more revealing. Can't imagine this happening to a male MP,"" she tweeted.

After Ms Purcell called it out, media outlets including CNN and the BBC picked up the story, fuelling debate on the reach of generative artificial intelligence in our lives.

In its swift apology to Ms Purcell, broadcaster Nine News said the alteration had occurred due to an ""automation by Photoshop"" while resizing the original photo.

Ms Purcell says she's not sure she buys Nine's explanation but is happy to move on, provided a lesson is learnt by everyone to ensure it never happens again.

So how might the image have been made and what lessons should we take from it?

Expert says Nine's explanation is plausible
Nine News has told the ABC the alteration to Ms Purcell's midriff occurred when using Adobe Photoshop's ""generative expand"" tool.

The tool allows users to make an image bigger — the program uses AI to make assumptions or guesses about how that image might be best filled out with new material.

Nine said the image it used to produce the graphic was a more tightly cropped version of the Bendigo Advertiser's photo of Ms Purcell.

This image appears in some online image searches, and is cropped above Ms Purcell's waist.

Georgie Purcell stands in front of a river.
Nine News says this version of the photo, which is cropped higher than the original, was the version ingested into its system. (Bendigo Advertiser)

When that image was dragged downwards using the expand tool, the generated image exposing her midriff was created, according to Nine.

This was the image later incorporated into a graphic used in the network's coverage of debate over the future of duck hunting in Victoria.

Adobe released a statement saying edits to the image in question ""would have required human intervention and approval"".

A image of a TV news anchor and a graphic of duck hunting.
This graphic, including the altered image of Ms Purcell, appeared briefly before Nine News's TV report on duck hunting on Monday. (Nine News)

TJ Thomson, a senior lecturer in digital media at RMIT, said it was plausible that a cropped image could produce a range of different torsos when expanded using AI.

""If you are giving Photoshop less data about the body … it has to imagine a lot more about what's below the torso,"" he said.

""So it can have a lot more creative input into what is below the torso.""

To demonstrate the tool in question, we uploaded a photo of a consenting ABC employee to Photoshop, cut off just above the chin.

When the program was presented with the image and asked to perform the ""generative expand"" action, it produced several variants of the torso.

An AI-generated image of a man, with three alternative torsos. His face is blurred.
An image of a man created with AI generation below the neck, creating three alternative torsos.  (ABC News)

The clothing in each torso differed, including a buttoned-up shirt and one with several of the top buttons undone.

In one version, the program appeared to struggle to generate the hands, presenting a jumble of fingers at the end of the man's arms.

How a 'vast library' of images informs generative AI
Technology researcher Shaanan Cohney, who lectures at the University of Melbourne's Centre for AI and Digital Ethics, said Photoshop's generative fill tool was able to create new material for an image by drawing on a ""vast library of stock images"".

""Now, Adobe claims to have done this in a way that reduces the bias in the images out there,"" Dr Cohney said.

""But without more information, it's very hard for us to know what kinds of stereotypes might be in that information.""

Dr Thomson said we could infer that many generative AI tools trawled the web to draw on images from films and pop culture, creating results that reinforced the biases present in society.

""If we look at our own landscape in terms of cinema or stock photography, often we see very biased and under-representative images and movies,"" he said.

TJ Thomson standing in front of a blurry background with a large green plant.
TJ Thomson says AI was being used as a tool to spread misinformation. (Supplied: Anthony Weate)

""Quite white-dominated, quite middle-aged or younger, not a lot of old folks, not a lot of people with disabilities.

""And so all those biases then are reproduced or enhanced sometimes with AI, if you're not particularly prompting to try to get the more diverse representation.""

In a statement, an Adobe spokesperson said the company was committed to principles of accountability, responsibility and transparency when developing its AI products.

""Generative Fill and Generative Expand in Photoshop is powered by Adobe Firefly, Adobe's family of creative generative AI models, which is trained on the highest quality of assets including Adobe Stock images, openly license content and public domain content where copyright has expired,"" they said.

'Sexualised' images of women are training AI, researchers warn
When Ms Purcell saw her photoshopped body displayed on the TV, she said it was deeply upsetting.

""I don't think there's a single young person that hasn't struggled with their body image, and seeing your own body altered on TV is very confronting,"" she said.

""This has affected me in some way, and it could affect other women even more. It should never happen again.""

Walkley-award winning journalist Tracey Spicer said when she heard about Ms Purcell's experience she was ""horrified, but not surprised"".

Tracey Spicer speaks to ABC News
Tracey Spicer says media companies have an obligation to better train staff. (ABC News: Jerry Rickard)

Spicer, who recently authored a book examining the rise of artificial intelligence, said generative AI ""routinely sexualises images of women and girls"".

""In fact, while designing the cover for my book, we put in prompts to design an image of a strong robot woman looking to the future with hope but concern,"" Spicer said.

""[The AI generator] instead created an image of a sexy gold robot with huge breasts and a tiny waist.""

Spicer also backed a warning from the Victorian Women's Trust that the community was on ""the verge of a tsunami"" when it came to the weaponisation of AI against women.

Singer and songwriter Taylor Swift was the latest high-profile woman to be attacked using AI, after pornographic deepfake images were circulated online.

Taylor Swift, wearing a purple dress, poses for a photo.
A sexually explicit image of Taylor Swift that was AI generated was viewed 47 million times before the account was suspended. (Reuters: Steve Marcus)

Spicer said as technology became freer and easier to use, its potential to be abused was growing.

""Frankly, it's terrifying,"" she said.

""Many victim-survivors of this kind of abuse say it's like being physically, sexually assaulted.""

New York University data journalism researcher Meredith Broussard said the doctored image of Ms Purcell was an important reminder that AI ""keeps failing and screwing up"" in a range of ways and human intervention was critical.

A woman with curly hair and hoop earrings stands in front of a bookshelf smiling.
New York University data journalism researcher Meredith Broussard says we must interrogate AI's assumptions. (Supplied: Devin Curry)

""It's a lesson in why we shouldn't necessarily trust the defaults in artificial intelligence,"" she said.

""People think that AI is going to be somehow superior to humans and that's simply not the case.""

In Ms Purcell's case, Dr Broussard said the person operating the program that generated the altered image should have spotted the changes and opted not to use that image.

'Major harms' occurring due to racial biases
In the United States, Dr Broussard said there were several examples of ""major harms"" being carried out against people as a result.

""Facial recognition systems tend to be better at recognising light skin than dark skin, they tend to be better at recognising men than recognising women, they tend to not take trans and non-binary folks into account at all,"" she said.

""And so when facial recognition is used in policing, for example, it is disproportionately weaponised against communities of colour.""

A man looks at a demonstration on a big screen of software tracking the movement of people.
Consumer group CHOICE raised concerns some retailers were using facial recognition technology without the knowledge of customers. (Reuters: Thomas Peter)

Dr Thomson said it was also clear that AI was being used as a tool to spread misinformation.

""People are using these tools to make photorealistic generations that can deceive people or mislead people,"" he said.

""We're seeing that happening in the Hamas, Israel conflict and multiple other contexts,"" he said.

In Australia, retail giants Bunnings and Kmart are being investigated over their use of facial recognition technology in stores, amid privacy concerns.

Media companies urged to offer AI training
So where does this leave the media with its use of AI?

Dr Thomson said there were a range of different approaches being taken by outlets around the world.

Think you can spot content written by AI?
Photo shows Conceptual image of a robot office workerConceptual image of a robot office worker
You may not have heard of GPT-3, but there's a good chance you've read its work, used a website that runs its code, or even conversed with it through a chatbot or a character in a game.

""The majority of folks I've talked to are most comfortable using these tools for things like illustrations, things that are definitely not photorealistic, where you don't have the potential to mislead or deceive,"" he said.

""If you're doing a story on mental health or COVID, or that kind of thing, and you have some clearly illustrated image, that's AI-generated, that's the kind of test case or use case that photo editors feel most comfortable using.

""Some outlets are a bit more conservative, saying we're only going to use AI-generated images when we're reporting on AI-generated images that have gone viral, for example.""

At the ABC, there is a policy not to use generative AI images in place of real images in news reporting.

If one is used, it must be clearly labelled — as we've done in this story.

Spicer said media companies had an obligation to better train staff.

""Issues like this will only deepen the public's scepticism about journalism. That's the last thing we need, at a time when democracy is under threat,"" she said.

""There are things that journalists and media organisations can be doing right now. Staff should be trained in how to use AI safely and ethically.""

Calls for greater regulation in Australia
Several experts agree greater regulation of AI is needed — particularly in Australia.

In January, the Australian government unveiled plans to target some high-risk AI technologies with audits and safeguards — but Dr Thomson said more was needed to ""catch up"" with the rest of the world.

""Australia has been called kind of the back of the pack in terms of AI regulation,"" Dr Thomson said.

We asked an AI tool to 'paint' images of Australia. Critics say they're good enough to sell
Photo shows A painting of a couple embracing at a beachA painting of a couple embracing at a beach
Computer programs, trained on the internet and only invented this year have created these artworks based on a few text prompts.

In December, the European Union passed the world's first AI law, which paved the way for legal oversight of technology used in popular generative AI services.

In the United States, President Joe Biden made a major effort to regulate artificial intelligence under a new executive order where AI developers would need to risk-test their tools and share the results with the US government.

On Wednesday, Australian Securities and Investments Commission chair Joe Longo warned existing laws did not adequately prevent ""AI-facilitated harms"" before they occurred and further ""transparency and oversight"" may be needed.

Another idea flagged by the federal government is tagging AI-generated images with a watermark or in another way that ensures people seeing them are aware of how they've been made.

For its part, Adobe said that was an idea the company supported, likening it to health advice included on food packaging.

""We are working with customers and partners to enable content credentials — which are ""nutrition labels"" for digital content, that remain associated with content wherever it is used, published or stored,"" a spokesperson said.

A view over a person's shoulder as they look at a digitally altered image of a politician.
Researchers say better AI literacy across the community will become increasingly important as the technology expands. (ABC News: Gabriela Rahardja)

Dr Cohney said even with legal obligations, ""ethical norms"" and ""AI literacy"" in the community would be critical as the technology's use expanded.

Even if we have the perfect set of laws, they still won't catch every example of people not using them appropriately,"" he said.

""So we need to think about the way we train people, whether that be in the media, or whether they be working in another industry, to use AI in an ethical fashion.""",,
18,Why Ethical AI Must Be A Leadership Priority,https://www.forbes.com/sites/jonathanreichental/2024/05/22/why-ethical-ai-must-be-a-leadership-priority/,Ned,News Article,"Why Ethical AI Must Be A Leadership Priority
Jonathan Reichental, PhD
Contributor
I write about the role of technology in business and society.

Just because we can do something with artificial intelligence doesn't mean we should do it.GETTY
42 percent of companies are using artificial intelligence (AI) and an additional 40 percent are exploring uses, reports IBM. To underestimate AI value and delay adoption may be an existential risk for many organizations. Equally, embracing it too quickly without adequate attention paid to matters such as fairness, privacy, and accountability risks unpredictable negative consequences. Rather than an afterthought, the journey to broad AI adoption must include ethical AI: the implementation and management of AI solutions that prioritize responsible uses and reduced risks for all stakeholders.

Preparing For Revolutionary Change
The rapid progress of AI and its transformational impact on all aspects of business operations is nothing short of revolutionary. The remarkable speed at which AI is developing has all the characteristics of a change without historic parallel. Much like recent transformational technologies such as the Internet and smartphones, AI will join and perhaps outpace them as a new context for reinvention.

There’s a lot to be excited about, but poor moves early in any adoption have the potential to derail efforts or worse still, be a destructive force both to the organization and society. Producing output with absent or misguided rules, for example, can produce reputational risks and embarrassing results.

Specifically, generative AI relies heavily on vast quantities of historical data harvested from across the public Web and other accessible repositories. This data coupled with algorithms shapes what AI produces. If either one or both are mismanaged, the negative consequences are all too real.

Suddenly good intentions produce novel and unintended issues of fairness, discrimination, trust, privacy, transparency, and more. AI raises complex questions about its appropriate use with many still largely unanswered. To proactively mitigate unwanted outcomes including legal risks requires an ethical approach to AI.

The Role Of Ethical AI
The purpose of ethical AI is to support the responsible design and development of AI solutions that protect individuals, groups, and society, from harm. To succeed requires a combination of efforts that include computer science, policy implementation, and governance. Importantly, it must be a leadership priority.

Organizations will be best served by treating ethical AI not as a nice-to-have, but rather, an essential part of their AI efforts. Diligence can help protect the business, enable greater confidence in deployments, and assist with compliance with existing or future requirements. In fact, an increasing volume of national and international laws and regulations will make aspects of ethical AI required for organizations. Business leaders must become aware of their current obligations and be on top of developments as they emerge.

Steps Organizations Can Take Now
For organizations just beginning their AI journey, knowing where to begin with ethical AI can seem overwhelming. It can be perceived as an unwelcome effort that will slow or even disincentivize innovation. Despite these concerns, the benefits of ethical AI will eventually easily outweigh them.

It’s never too soon to begin implementing an approach to ethical AI and that includes those well into their AI developments and deployments. Here are three ways that all types of businesses can begin to increase their responsible use of AI.

Establish AI principles and standards: Identify what is most important to the organization, customers, products, and services, and use those to guide decisions.
Implement AI policies and procedures: Clearly outline how the organization will implement and enforce its principles and standards.
Create an AI governance capacity: Define how the organization will provide ongoing oversight for AI activities including monitoring efforts, providing training, and continuous improvement.
For too many, implementing ethical AI may not be a top-of-mind priority today, but it must become one. Leaders need to be convinced that doing the right thing with AI will get the right outcomes.

Creating and implementing a viable approach to ethical AI offers the best chance of avoiding significant issues. Without ethical AI, organizations may think they are moving in the fast lane only to wake up and find they are operating in the wrong lane.",,
20,The Ethical Dilemma Of AI In Marketing: A Slippery Slope,https://forbes.com/sites/elijahclark/2024/03/14/the-ethical-dilemma-of-ai-in-marketing-a-slippery-slope/,Ned,News Article,"The Ethical Dilemma Of AI In Marketing: A Slippery Slope


Artificial Intelligence has revolutionized the marketing landscape, offering businesses unprecedented insights into consumer behavior and preferences. However, as AI becomes increasingly intertwined with marketing strategies, the question of ethics becomes more pressing. Can AI be genuinely ethical when used in marketing, or is it inherently prone to manipulation and deception?
The ethical implications of AI in marketing were recently discussed at the annual Business School DEI Collaborative Conference hosted by Texas Christian University. The conference brought together experts from academia and industry to explore the intersection of AI and DEI in business schools. During the session, the question of whether a business could be ethical when using AI in marketing was raised, highlighting the growing concern about the potential misuse of this powerful technology.
At its core, AI is a tool that relies on consumer data input. Just as a weapon is not inherently dangerous without human intervention, AI's ethical implications are primarily determined by the data it is fed and the intentions of those wielding it. Unfortunately, the marketing industry has a history of pushing the boundaries of ethics in pursuit of profit. From misleading advertisements to artificial ingredients, marketing has often prioritized sales over transparency and consumer well-being.
Politicians often use manipulative tactics and messaging in their marketing strategies, similar to how businesses may use AI for unethical purposes in marketing. Some candidates exaggerate or fabricate data and information during political campaigns to gain support. These politicians aim to polarize the electorate by tapping into voters' anxieties and prejudices, positioning themselves as the only solution to perceived problems. This strategy is similar to how AI algorithms can exploit consumer vulnerabilities and desires to influence purchasing decisions. Businesses may use AI to target individuals with personalized marketing messages that play on their insecurities or impulses, and politicians can craft their messaging to appeal to voters' deepest fears and biases.
The use of AI in marketing raises concerns about the potential for even more sophisticated forms of manipulation. With access to vast amounts of consumer data, AI algorithms can create highly personalized marketing messages that exploit individuals' vulnerabilities and desires. As a result, consumers may be swayed to make purchases that are not in their best interests or even harmful to their health and well-being.
Moreover, the opaque nature of AI algorithms makes it difficult for consumers to understand how their data is being used and how marketing messages are being tailored to them. This lack of transparency creates an uneven playing field, where businesses have an unfair advantage over consumers unaware of how much their behavior is being monitored and influenced.
The ethical concerns surrounding AI in marketing are not limited to consumer manipulation. There are also questions about the potential for AI to perpetuate biases and discrimination. If the data used to train AI algorithms is biased, the resulting marketing messages may reinforce harmful stereotypes and exclude certain groups of people. This is particularly concerning given the historical underrepresentation of marginalized communities in marketing and the potential for AI to exacerbate these inequities.
Furthermore, using AI in marketing raises questions about the erosion of human agency and free will. As AI becomes more sophisticated at predicting and influencing consumer behavior, individuals may find it increasingly difficult to make autonomous choices. This could lead to a future where our desires and preferences are shaped by algorithms rather than our own values and beliefs.
To address these ethical concerns, businesses would need to prioritize transparency and accountability in their use of AI for marketing purposes. Consumers should be fully informed about how their data is being collected and used and given the opportunity to opt out of targeted marketing. Additionally, businesses should be held responsible for any harm caused by their AI-driven marketing practices, whether through the spread of misinformation or the exploitation of vulnerable populations.
Ultimately, whether AI can be ethical in marketing is a complex question with no easy answers. While AI has the potential to provide valuable insights and improve the efficiency of marketing campaigns, it also carries significant risks of manipulation, discrimination, and the erosion of human agency. As we navigate this new frontier of marketing technology, it is crucial that we remain vigilant in our efforts to ensure that the use of AI aligns with our values and promotes the well-being of both consumers and society as a whole.",,
22,This might be the most important job in AI,https://www.businessinsider.com/ai-chief-ethics-officer-2024-7,Ned,Industry Article,"This might be the most important job in AI
Lakshmi Varanasi Jul 21, 2024, 7:22 PM GMT+10

Share

Save
Name plate photo illustration.
There's an exciting new job in tech at the intersection of policy, ethics, and AI. Jenny Chang-Rodriguez/BI
A company's chief ethics officer ensures AI is used responsibly.
They define principles for regulating the tech, learn the legal landscape, and liaise with stakeholders.
Those in the role often earn annual salaries in the mid-six figures. 
Insider Today
Sign up to get the inside scoop on today’s biggest stories in markets, tech, and business — delivered daily. Read preview
Email address
Enter your email
Sign up
By clicking “Sign Up”, you accept our Terms of Service and Privacy Policy. You can opt-out at any time by visiting our Preferences page or by clicking ""unsubscribe"" at the bottom of the email.
Bull
The launch of ChatGPT ushered the corporate world into a new era.

The buzzy bot's technology — generative AI — could write emails, produce code, and materialize graphics in minutes. Suddenly, the days in which workers pored over their inboxes and painstakingly crafted presentations seemed like a relic of the past.

Companies, lured by profit and productivity gains, rushed to adopt the technology. According to a May survey from consulting firm Mckinsey & Company, 65% of the more than 1,300 companies it researched said they now regularly use generative AI — double the number using it the year before.

But the risks of misusing the technology loom large. Generative AI can hallucinate, spread misinformation, and reinforce biases against marginalized groups if it's not managed properly. Given that the technology relies on volumes of sensitive data, the potential for data breaches is also high. At worst, though, there's the danger that the more sophisticated it becomes, the less likely it is to align with human values.

With great power, then, comes great responsibility, and companies that make money from generative AI must also ensure they regulate it.

That's where a chief ethics officer comes in.

A critical role in the age of AI
The details of the role vary from company to company but — broadly — they're responsible for determining the impact a company's use of AI might have on the larger society, according to Var Shankar, the chief AI and privacy officer at Enzai, a software platform for AI governance, risk, and compliance. ""So beyond just your company and your bottom line, how does it affect your customers? How does it affect other people in the world? And then how does it affect the environment,"" he told Business Insider. Then comes ""building a program that standardizes and scales those questions every time you use AI.""

It's a role that gives policy nerds and philosophy majors, alongside programming whizzes, a footing in the fast-changing tech industry. And it often comes with a sizable annual paycheck in the mid-six figures.

Right now, though, companies aren't hiring people into these roles fast enough, according to Steve Mills, the chief AI ethics officer at Boston Consulting Group. ""I think there's a lot of talk about risk and principles, but little action to operationalize that within companies,"" he said.

A C-suite level responsibility
Those who are successful in the role ideally have four areas of expertise, according to Mills. They should have a technical grasp over generative AI, experience building and deploying products, an understanding of the major laws and regulations around AI, and significant experience hiring and making decisions at an organization.

Related stories

She interviewed for 15 AI roles before landing a Microsoft offer. It made her realize how the job market has changed.


'Botshit' is an example of how AI is making customer service worse

""Too often, I see people put midlevel managers in charge, and while they may have expertise, desire, and passion, they typically don't have the stature to change things within the organization and rally legal, business, and compliance teams together,"" he said. Every Fortune 500 company using AI at scale needs to charge an executive with overseeing a responsible AI program, he added.

Shankar, a lawyer by training, said that the role doesn't warrant any specific educational background. The most important qualification is understanding a company's data. That means having a handle on the ""ethical implications of the data that you collect, use, where it comes from, where it was before it was in your organization, and what kinds of consent you have around it,"" he said.

He pointed to the example of healthcare providers who could unintentionally perpetuate biases if they don't have a firm grasp of their data. In a study published in Science, hospitals and health insurance companies that used an algorithm to identify patients that would benefit from ""high-risk care management"" ended up prioritizing healthier white patients over sicker black patients. That's the kind of blunder an ethics officer can help companies avoid.

Collaborating across companies and industries
Those in the role should also be able to communicate confidently with various stakeholders.

Christina Montgomery, IBM's vice president, chief privacy and trust officer, and chair of its AI Ethics Board, told BI that her days are usually packed with client meetings and events, alongside other responsibilities.

""I spent a lot of time externally, probably more time lately, in speaking at events and engaging with policymakers and on the external boards because I feel like we have very much an opportunity to influence and determine what the future looks like,"" she said.

She sits on boards like the International Association of Privacy Professionals, which recently launched an Artificial Intelligence Governance Professional certification for individuals who want to lead the field of AI ethics. She also engages with government leaders and other chief ethics officers.

""I think it's absolutely critical that we be talking to each other on a regular basis and sharing best practices, and we do a lot of that across companies,"" she said.

She aims to develop a broader understanding of what's happening on a societal level — something she sees as key to the role.

""My fear at the space that we are at right now is that there's no interoperability globally among all these regulations, and what's expected, and what's right and wrong in terms of what companies are going to have to comply with,"" she said. ""We can't operate in a world that way. So the conversations among companies, governments, and boards are so important right now.""",,
23,Are tomorrow’s engineers ready to face AI’s ethical challenges?,https://theconversation.com/are-tomorrows-engineers-ready-to-face-ais-ethical-challenges-213826,Ned,Research Paper,"Are tomorrow’s engineers ready to face AI’s ethical challenges?
A chatbot turns hostile. A test version of a Roomba vacuum collects images of users in private situations. A Black woman is falsely identified as a suspect on the basis of facial recognition software, which tends to be less accurate at identifying women and people of color.

These incidents are not just glitches, but examples of more fundamental problems. As artificial intelligence and machine learning tools become more integrated into daily life, ethical considerations are growing, from privacy issues and race and gender biases in coding to the spread of misinformation.

The general public depends on software engineers and computer scientists to ensure these technologies are created in a safe and ethical manner. As a sociologist and doctoral candidate interested in science, technology, engineering and math education, we are currently researching how engineers in many different fields learn and understand their responsibilities to the public.

Yet our recent research, as well as that of other scholars, points to a troubling reality: The next generation of engineers often seem unprepared to grapple with the social implications of their work. What’s more, some appear apathetic about the moral dilemmas their careers may bring – just as advances in AI intensify such dilemmas.

Make better decisions - find out what the experts think.
Aware, but unprepared
As part of our ongoing research, we interviewed more than 60 electrical engineering and computer science masters students at a top engineering program in the United States. We asked students about their experiences with ethical challenges in engineering, their knowledge of ethical dilemmas in the field and how they would respond to scenarios in the future.

First, the good news: Most students recognized potential dangers of AI and expressed concern about personal privacy and the potential to cause harm – like how race and gender biases can be written into algorithms, intentionally or unintentionally.

One student, for example, expressed dismay at the environmental impact of AI, saying AI companies are using “more and more greenhouse power, [for] minimal benefits.” Others discussed concerns about where and how AIs are being applied, including for military technology and to generate falsified information and images.

When asked, however, “Do you feel equipped to respond in concerning or unethical situations?” students often said no.

“Flat out no. … It is kind of scary,” one student replied. “Do YOU know who I’m supposed to go to?”

Another was troubled by the lack of training: “I [would be] dealing with that with no experience. … Who knows how I’ll react.”

Two young women, one Black and one Asian, sit at a table together as they work on two laptops.
Many students are worried about ethics in their field – but that doesn’t mean they feel prepared to deal with the challenges. The Good Brigade/DigitalVision via Getty Images
Other researchers have similarly found that many engineering students do not feel satisfied with the ethics training they do receive. Common training usually emphasizes professional codes of conduct, rather than the complex socio-technical factors underlying ethical decision-making. Research suggests that even when presented with particular scenarios or case studies, engineering students often struggle to recognize ethical dilemmas.

‘A box to check off’
Accredited engineering programs are required to “include topics related to professional and ethical responsibilities” in some capacity.

Yet ethics training is rarely emphasized in the formal curricula. A study assessing undergraduate STEM curricula in the U.S. found that coverage of ethical issues varied greatly in terms of content, amount and how seriously it is presented. Additionally, an analysis of academic literature about engineering education found that ethics is often considered nonessential training.

Many engineering faculty express dissatisfaction with students’ understanding, but report feeling pressure from engineering colleagues and students themselves to prioritize technical skills in their limited class time.

Researchers in one 2018 study interviewed over 50 engineering faculty and documented hesitancy – and sometimes even outright resistance – toward incorporating public welfare issues into their engineering classes. More than a quarter of professors they interviewed saw ethics and societal impacts as outside “real” engineering work.

About a third of students we interviewed in our ongoing research project share this seeming apathy toward ethics training, referring to ethics classes as “just a box to check off.”

“If I’m paying money to attend ethics class as an engineer, I’m going to be furious,” one said.

These attitudes sometimes extend to how students view engineers’ role in society. One interviewee in our current study, for example, said that an engineer’s “responsibility is just to create that thing, design that thing and … tell people how to use it. [Misusage] issues are not their concern.”

One of us, Erin Cech, followed a cohort of 326 engineering students from four U.S. colleges. This research, published in 2014, suggested that engineers actually became less concerned over the course of their degree about their ethical responsibilities and understanding the public consequences of technology. Following them after they left college, we found that their concerns regarding ethics did not rebound once these new graduates entered the workforce.

Joining the work world
When engineers do receive ethics training as part of their degree, it seems to work.

Along with engineering professor Cynthia Finelli, we conducted a survey of over 500 employed engineers. Engineers who received formal ethics and public welfare training in school are more likely to understand their responsibility to the public in their professional roles, and recognize the need for collective problem solving. Compared to engineers who did not receive training, they were 30% more likely to have noticed an ethical issue in their workplace and 52% more likely to have taken action.

An Asian man wearing glasses stares seriously into space, standing against a holographic background in shades of pink and blue.
The next generation needs to be prepared for ethical questions, not just technical ones. Qi Yang/Moment via Getty Images
Over a quarter of these practicing engineers reported encountering a concerning ethical situation at work. Yet approximately one-third said they have never received training in public welfare – not during their education, and not during their career.

This gap in ethics education raises serious questions about how well-prepared the next generation of engineers will be to navigate the complex ethical landscape of their field, especially when it comes to AI.

To be sure, the burden of watching out for public welfare is not shouldered by engineers, designers and programmers alone. Companies and legislators share the responsibility.

But the people who are designing, testing and fine-tuning this technology are the public’s first line of defense. We believe educational programs owe it to them – and the rest of us – to take this training seriously.",,
25,Why Pope Francis thinks the Church should play a part in world leaders’ debate on AI,https://edition.cnn.com/2024/06/12/tech/pope-francis-g7-artificial-intelligence-intl-cmd/index.html,Dan,News Article,"Why Pope Francis thinks the Church should play a part in world leaders’ debate on AI
By Christopher Lamb, CNN

Pope Francis attends the weekly general audience in Saint Peter's Square at the Vatican, June 5, 2024.
Pope Francis attends the weekly general audience in Saint Peter's Square at the Vatican, June 5, 2024. 

An image of Pope Francis wearing a stylish white puffer jacket went viral last year, prompting a flurry of comments about his choice of clothes and even questions about whether he had a stylist. But there was a problem: the image was a “deep fake” created using artificial intelligence.

This week, the pope is due to make an historic intervention in the debate around AI at the G7 summit in southern Italy’s Puglia region. On Friday, Francis will become the first pope to participate in the summit of leaders from the world’s most advanced economies when he takes part in a session dedicated to AI. US President Joe Biden, a Catholic who has a warm relationship with Francis, is among the leaders expected to be present at the gathering.

The 87-year-old pontiff is determined to use the soft power of his office to try to ensure that the development of AI serves humanity and does not turn into a 21st-century Frankenstein’s monster.

For the pope, who as a young man trained as a chemist, developments in science and technology are to be welcomed; he believes AI offers exciting new opportunities. But the pope also foresees some grave risks.

In a message released late last year, he warned that a “technological dictatorship” could emerge if sufficient regulation was not put in place, highlighting the threats posed by AI-controlled weapons systems and the dangers that technology could be misused for a surveillance society and interference in elections. AI, the pope believes, can make the world a better place only if it serves the “common good” and does not increase inequalities.

The pope and the Vatican have been pushing for an ethical framework to underpin the development and use of AI. Since 2020, the Vatican’s Pontifical Academy for Life, a body advocating for Catholic moral teachings on bioethics, has been promoting “Rome Call for AI Ethics,” a document setting out six principles for AI ethics, among them transparency, inclusion, responsibility and impartiality.

The Vatican is seeking buy-in from big tech companies and governments. So far, the document has been signed by Microsoft (MSFT), IBM (IBM), and Cisco Systems (CSCO), along with the United Nations’ Food and Agriculture Organization, Italy’s innovation ministry and numerous religious leaders.

At the G7 summit, the pope is expected to urge world leaders to work together on the regulation of AI, echoing his call at the end of last year for a “binding international treaty” to prevent harmful practices and encourage the best ones. European Union lawmakers have already passed a law regulating AI, while a bipartisan group of US senators have set out plans for AI regulation that could lead to federal legislation.

“The pope is not an engineer, but he is concerned about the social aspects and implications of AI,” Father Paolo Benanti, a Franciscan friar and professor who has been working with the Vatican on the issue of AI, told CNN. Benanti is also a member of a UN advisory body on AI. At the G7, he expects the pope to emphasize elements of his previous messages on the subject.

“The core approach of Francis is focused on what new technology means for our co-existence: which elements of AI are causing inequality for humanity, and topics such as the distribution of fake news in the public square. He takes a global outlook and sees that the global south does not have the same access to technology as other parts of the world.”

Benanti said Francis was sensitive to the “great challenges facing humanity,” noting that he had begun his papacy by highlighting the plight of migrants. He has also addressed the threat posed by climate change in a major papal document and is now focused on AI, Benanti added.

The AI-generated image was made using a tool called Midjourney.
The AI-generated image was made using a tool called Midjourney. From Midjourney
Archbishop Vincenzo Paglia, president of the Pontifical Academy for Life, told CNN that “only regulation at the international level can produce valid and appreciable results in stopping abuse, manipulation and instrumentalization” of new technology. The academy’s push for a more ethical AI, he added, seeks to ensure a “path of sustainable development for all humanity.”

Italy, which currently holds the rotating presidency of the G7, last year placed a temporary ban on ChatGPT, a chatbot and virtual assistant, over privacy concerns and has plans to penalize the misuse of AI. Giorgia Meloni, Italy’s prime minister, has said she believes the pope’s presence in Puglia “will give a decisive contribution to drawing up an ethical and cultural regulatory framework” for AI. She said it was crucial to harness the “best ethical and intellectual reflections” in this area, adding that the “Rome Call for AI Ethics” was helping create the idea of “algorethics” — ethics for algorithms.

Francis’ decision to become the first pope to participate in a G7 summit signals his desire to be where the “real debate actually takes place,” papal adviser Father Antonio Spadaro posted on X, formerly Twitter. While in Puglia, Francis will have the chance to talk directly with decision-makers, and his decision underlines the pope’s vision of a Church that engages with the world rather than retreating from it.

Father Philip Larrey, the author of a book on AI, “Artificial Humanity,” and former dean of the Philosophy Department at the Pontifical Lateran University in Rome, described Francis’ decision to attend as “quite surprising” but one Larrey believes will “influence the outcome” of the summit.

“AI and emerging technologies are on Pope Francis’ radar screen,” Larrey, now a professor of philsophy at Boston College, told CNN. “(He) wants to use the richness of the Catholic tradition in order to weigh in on the importance of reflecting on the ethical implications of AI. And his personal presence in (Puglia) testifies to the urgency of that message: he often refers to ‘person-centered AI’ to make his point.”

The “deep fake” image of the pope in the puffer jacket became a landmark moment for the development of AI and deep fakery, showcasing the power of new technology to manipulate images.

Francis addressed this when warning earlier this year about disinformation and the spread of images that “appear perfectly plausible but false.” He pointed out: “I too have been an object of this.”

It wasn’t just the puffer jacket image: the pope has repeatedly been the subject of deep fakery, with computer-generated images circulating of him skateboarding, riding a motorcycle and even blending in at the Burning Man festival in Nevada.

It’s clear that Francis sees AI as part of what he called the “epochal change” taking place at the beginning of the 21st century.

His decision to attend the G7 summit indicates he wants the Church to be at the heart of discussions about how this change unfolds, and to help ensure that new technology can benefit the whole of humanity.",,
37,Pope Francis to weigh in on 'ethical' AI at G7 summit,https://www.news.com.au/breaking-news/pope-francis-to-weigh-in-on-ethical-ai-at-g7-summit/news-story/3833539f0a157776b06d2393ae433dfa,Ned,News Article,"Pope Francis to weigh in on 'ethical' AI at G7 summit. Pope Francis will address G7 leaders on Friday on artificial intelligence, an unprecedented appearance that reflects the Vatican's growing interest in the new technology, its risks and rewards.

The 87-year-old will become the first head of the Catholic Church to address a G7 summit when he speaks on the second day of the Puglia meeting, to an audience including US President Joe Biden and France's Emmanuel Macron.

The ageing head of a 2,000-year-old institution is not perhaps the most obvious candidate to make a presentation on cutting-edge technology, but the pontiff sees AI as a key challenge for humanity.

""The Church always looks to humans as the centre of its mission,"" said Paolo Benanti, a Franciscan university professor and member of the UN's AI advisory body, who directly advises the pope.

""From this perspective it is clear that the AI that interests the Church is not the technical tool, but how the tool can impact on the life of man,"" he told AFP.

AI was the theme of the Church's World Day of Peace on January 1, for which the pontiff published a six-page document.

In it, he welcomed advances in science and technology that have reduced human suffering -- and Benanti said AI could act as a ""multiplier"", boosting everything from medical research to economic and social wellbeing.

But the pope also warned of risks including disinformation and interference in elections, and that unequal access could increase social and economic inequalities.

Francis -- who has himself been the subject of several AI-generated images, including a viral imagine showing him wearing a huge white puffer coat and a large crucifix -- called for a binding international treaty to regulate the development and use of AI.

The goal would be to prevent harm and share good practice. 

- 'Human-centred approach' -

Since the launch of OpenAI's ChatGPT chatbot, whose capabilities range from digesting complex text to writing poems and computer code, governments have been scrambling to respond to the rapid growth of AI.

The European Union -- which attends G7 summits as an unofficial eighth member -- earlier this year approved the world's first comprehensive rules to govern AI.

At a global level, G7 leaders in Japan last year announced a working group on AI's ""responsible"" use, tackling issues from copyright to disinformation.

Hosts Italy have made AI a key issue of this year's summit, which will focus on a ""human-centred approach"", particularly its potential impact on jobs, according to a government source.

Italian Prime Minister Giorgia Meloni said in April that the pope's presence would ""make a decisive contribution to defining a regulatory, ethical and cultural framework"".

The Vatican has brought in a range of experts to help its understanding, including Demis Hassabis, head of Google DeepMind, whom it named to its scientific academy in March.

In 2020, it also initiated the Call for AI Ethics, backed by tech firms Microsoft and IBM and later Cisco as well as numerous universities and the UN, designed to promote an ethical approach.

The pope's address on Friday is likely to call for ""attention to be paid to the most vulnerable"", said Eric Salobir, a French priest and head of the executive committee of the Human Technology Foundation.

It would be a call to G7 leaders to take ""into account the risks and (draw up) regulation without being alarmist"", he told AFP.

- No Vatican tech -

Francis, who has championed the poorest and most marginalised people in society since taking office in 2013, has cautioned that AI offers new freedoms but also the risk of a ""technological dictatorship"".

He warned about the dangers of using AI to make important decisions -- from social security payments to where to aim autonomous weapons -- for which responsibility becomes blurred.

""The pope seems to have a sort of antenna that allows him to perceive where humanity experiences the situations of greatest challenge to itself,"" Benanti said.

But will the G7 leaders listen to the pope?

Salobir, author of a book ""God and Silicon Valley"", says that besides his influence as a spiritual leader, the pope has power as a neutral observer.

""The fact that there is no 'Vatican Tech' is an asset in terms of neutrality -- the Church has no hidden agenda, no digital economy, no 'start-up nation' to launch, or investments to attract,"" he said.

As a result, when the Vatican talks about AI, ""it is for the technology itself, what it can do for humans"", he said.

""It may be one of the only states in this situation.""",,
39,AI ethical review should empower innovation—not prevent it,https://www.fastcompany.com/91069648/ai-ethical-review-should-empower-innovation-not-prevent-it,Rhea,Industry Article,"AI ethical review should empower innovation—not prevent it
When done appropriately, a robust AI ethics review board and process can serve as indispensable tools for companies aiming to adopt a comprehensive approach to responsible innovation.

AI ethical review should empower innovation—not prevent it

As companies rush to join the AI race and harness the technology’s potential to shape the future, a critical question looms large: Can ethics keep up with innovation?

While I’m particularly excited about generative AI’s power to transform creativity and productivity everywhere, I recognize that without appropriate oversight, this potentially revolutionary technology can also present real threats and hard challenges. As AI becomes a cornerstone of innovation across industries, it’s increasingly clear that companies have a duty to mitigate the risks it could pose. The journey towards responsible innovation must begin inside the companies that are driving innovation, calling for deep and wide-ranging collaboration from diverse stakeholders across the business.

One highly promising development is the growing embrace of an AI ethics review process, helmed by an AI ethics review board. Many companies establish these boards to ensure new products and technologies are developed responsibly and can safeguard against potential harm. When done appropriately, a robust AI ethics review board and process can serve as indispensable tools for companies aiming to adopt a comprehensive approach to responsible innovation. Still, work remains especially regarding the efficacy of AI ethics review boards in guiding the development of AI-powered products and features.

Many companies wrongly center their AI ethics review process on a stagnant board, channeling the company’s responsible innovation decisions to a singular entity. This approach causes obstacles that halt the pace of innovation and rely on perfunctory guardrails that serve only an abstract greater good. Other AI review boards act as strict regulatory bodies that set rigid guidelines and boundaries around what is off limits for the development of product or features, inhibiting discovery and stifling creativity from the ideation stage of a product lifecycle.

In the beginning, the AI review board should drive the design and execution of the company’s approach to an ethics review process. However, once these norms are established, then an AI ethics team can help operationalize the charter that the board has set. At Adobe, what we’ve seen is that a diverse AI ethics review process, armed with shared principles and tactical approaches to thoughtful innovation, can be a powerful catalyst for innovation. Further, a dedicated AI ethics team can empower teams to explore new frontiers without compromising ethical standards.

THE ROLE OF AN AI ETHICS BOARD: GUIDING WITH PRECISION
With the rise of generative AI, there lies an immense opportunity to amplify human creativity, not replace or diminish it. We believe AI review boards can further this opportunity when they serve as a guardian for innovation, not a gatekeeper. The focus of an AI review board should not lie in what companies can or should not do with AI technology, but instead, in figuring out how best to assess and mitigate risks so that internal teams can pursue their boldest dreams in the most ethical way.

To evaluate risk, Adobe created an AI Ethics Impact Assessment, led by our AI ethics team, for new AI-powered features—designed to identify features and products that can perpetuate harmful biases and stereotypes. In most cases, products show no major ethical concern and meet our standards for approval. If the assessment shows a higher potential ethical impact, further technical review is needed including a presentation to the AI review board who can ask questions and share improvements to mitigate risks of a product. With a robust assessment process already in place to guide responsible innovation from the outset, presentations to an AI review board should be minimal and reserved for addressing the most challenging issues. 

Championing a risk-based approach rather than a constrained one creates a playground for innovation. When teams are confident that an AI ethics team will support them in managing potential risks, it builds a trust-based culture that encourages stakeholders across the business—from senior leaders to product developers to marketers—to view ethics as a crucial component of their creative process. This allows boards to forgo granular reviews and concentrate on assessing the highest-risk technologies without slowing down the pace of innovation.

DIVERSE OUTCOMES NECESSITATE DIVERSE PERSPECTIVES
AI systems are only as inclusive as the data they are trained on, and the same principle applies to the diversity of the individuals overseeing the development of AI technologies. Diversity within AI review is paramount, as it ensures a wide array of perspectives in risk assessment and decision-making processes. As companies scale and introduce new products and features, adjustments to the AI review process become necessary. To match, the board should periodically refresh its membership to bring in fresh perspectives, aligning with the evolving needs of the business.

For instance, during the initial stages of forming your AI ethics review board and assessment process, prioritizing technical expertise is crucial for in-depth discussions on product advancement. This is not a nice-to-have. It’s a must-have to ensure that ethics are baked into product development from the onset. As products evolve towards market readiness, it’s important to expand the reviewers to encompass diverse viewpoints from various sectors within the company, including marketing, legal, and HR. This ensures comprehensive consideration of implications for both end-users and employees alike.

Incorporating members that embody not only a diversity of ethnicity, gender and sexual orientation, but also a diversity of thought and professional experience is critical for identifying potential issues in a product or feature.

ESTABLISHING SUSTAINABLE PRACTICES FOR INNOVATION
It is essential that companies build a review and oversight process for products with significant ethical stakes grounded in shared organization principles. Leveraging AI ethics assessments can enable companies to fully harness the power of AI while simultaneously cultivating and upholding trust among both employees and customers. Establishing guiding principles for ethical innovation provides a robust and unwavering foundation for boards as they navigate complex decisions and assess potential risks. When ethics are ingrained into design principles, responsible innovation becomes a natural part of a company’s DNA.

Our review process is firmly rooted in the principles of accountability, responsibility, and transparency, which we established in 2019. These principles are ingrained in every stage of product development, from design to deployment, including training and testing. To champion these principles, we rely on the diverse range of leaders and backgrounds within our AI review board, along with evangelists at every level and with perspectives from across our business to promote and uphold these values. We believe our principles are tactical, actionable, and broad enough to allow innovation to thrive. And even after a product launch, we continuously gather customer feedback to ensure our AI models are delivering outputs that are aligned to our customers’ needs.

Embarking on the journey of responsible innovation is challenging, yet imperative. While even the most efficient AI ethics review assessment cannot foresee and mitigate every risk, they play a crucial role in building trust within your business and with your customers. In the event that an unforeseen risk emerges, having an AI review process and strong principles integrated throughout your organization assures stakeholders that you took every possible step to reduce harm.

Companies pursuing ethical approaches to AI do not need to aim to change the world. Instead, they should focus on making tangible impact and embedding their principles into the aspects within immediate reach: their products, people, and purpose.",,
45,Dell teams up with governments worldwide to tackle issues of AI ethics,www.business-standard.com/companies/news/dell-teams-up-with-governments-worldwide-to-tackle-issues-of-ai-ethics-124052300216_1.html,Rhea,News Article,"Dell teams up with governments worldwide to tackle issues of AI ethics
Dell Technologies is actively collaborating with governments to establish appropriate policies and procedures, ensuring alignment with ethical standards within its probability models
Rimjhim Singh New Delhi


US tech major Dell Technologies is collaborating with governments around the world to address ethical issues surrounding artificial intelligence (AI). The partnerships are aimed at combating deepfakes, fraudulent activities, and biases, according to a report in the Economic Times.
The report quoted Jeff Boudreau, chief AI officer at Dell Technologies, as saying, “With vast amounts of data, the attack surfaces increase dramatically. There are good actors in the world, and there are bad actors in the world. Our role in collaboration with governments, different governments, is to balance innovation and technology, and doing the right things using the technology.”
Click here to connect with us on WhatsApp
Boudreau further said that Dell’s objective is to harness technology for positive impact. Additionally, the company is actively collaborating with governments to establish appropriate policies and procedures, ensuring alignment with ethical standards within its probability models.
Combating AI misuse
Boudreau’s remarks come at a time when there is a swift rollout of generative AI, resulting in the proliferation of deepfakes and the spreading of misinformation within the industry, the report stated.
Boudreau said the company is also collaborating with government agencies, such as those in Italy, the United States, or France. “But then also how do you use the technology to fight this? We have some customers tackling bad actors who are already using GenAI to, for example, conduct insurance fraud,” he said.
He said the use cases of GenAI among customers range from determining if individuals have purposely damaged one’s vehicles to filing fraudulent insurance claims. “The good news is, this technology is great at finding that as well. So you can recognise if it was altered or not,” Dell’s chief AI officer said.
More From This Section


Earlier this week, Dell introduced a lineup of AI-enhanced PCs featuring Qualcomm processors.
Additionally, it announced plans for a new server compatible with Nvidia’s latest chips, slated for release in the second half of 2024, as Dell seeks to sustain its presence in the lucrative AI server market and prepares for an anticipated rebound in PC demand after a post-pandemic slump in orders.",,
46,The Ethics Of AI: Balancing Innovation With Responsibility,https://www.forbes.com/sites/forbestechcouncil/2024/02/08/the-ethics-of-ai-balancing-innovation-with-responsibility/,Lida,News Article,"The Ethics Of AI: Balancing Innovation With Responsibility
Forbes Technology Council
Aleksandrs Malins
Former Forbes Councils Member
Forbes Technology Council

Founder and CEO of FortySeven Software Professionals, with over a decade of experience advising F500 companies and growth-stage startups.

 
In the dynamic world of technology, or the epoch of Industry 4.0 as it’s also commonly referred to, innovation knows no limits. Day by day, we witness changes happening in many services due to the implementation of AI and ML. If you don’t follow the news of new AI products even for one day, you might miss something important.

As a seasoned entrepreneur with more than 15 years of experience in IT consulting, I have noticed that we should delve into the ethics of AI implementation. There's no doubt that AI is providing us with numerous challenges and exciting opportunities along the way; however, we should also think about the future.

Exploring The Ethics Of AI
Many of us work extensively with our customers, implementing AI into existing business models across various industries. We all know that AI has the power of a revolutionary force, but at the same time, it raises ethical issues in many aspects, from biased algorithms to privacy concerns.

Currently, there are several approaches to AI ethics. For example, UNESCO adopted its ""Recommendation on the Ethics of Artificial Intelligence"" in November 2021, and the European Union issued its own document in 2020.

What if we consider the present? We are in 2024, and many innovations have already been applied in all industries. I believe there should be more amendments brought into existing recommendations of AI ethics.

Adoption Of AI In Companies
According to a report from McKinsey, ""60 percent of organizations with reported AI adoption are using gen AI"" and ""reported overall AI adoption remains steady at around 55 percent.""

What does that mean? From my perspective, it means that business models are changing, and there will be great challenges in almost every company in the near future. Yes, it brings innovation, but at the same time, we need to focus on new methodologies and policies inside the companies.

For instance, making it mandatory for companies to reveal the use of AI systems in decision-making can increase transparency and enable users to understand how AI affects them. Industry experts, researchers, entrepreneurs and policymakers must work together to create regulations that promote innovation while upholding ethical AI standards.

AI can have a significant impact in various situations. For example, there are companies in which AI models are used for decision-making in hiring employees and approving loans. However, sometimes AI models can create unfair outcomes.

I'm also concerned about privacy issues because AI collects a lot of personal information that can be used improperly. When working closely with engineers and ML specialists, we see how many threats there are that can be sensitive to privacy issues. This emphasizes the need for clear regulations in all industries, as well as transparency for all players in the market, including customers, engineers and business owners. We need to consider human rights as an important point.

Let’s also think about the fact that some AI products need to be used by specialists. On one hand, AI will eliminate many jobs; on the other hand, it will create a different kind of specialist that we didn't see previously. According to a report by Goldman Sachs, AI has the potential to replace around 300 million full-time jobs. This could involve automating a quarter of work tasks in the U.S. and Europe.

I've heard some specialists say that we are going to face new professions such as AI Virtual Reality Architect, AI Creativity Consultant, AI Supply Chain Analyst, AI Cybersecurity Analyst and many more. The report also suggests that AI could contribute to a 7% increase in the total annual value of global goods and services. Just like with a glass, some people see it as half empty while others see it as half full. That’s why it has become crucial to think about regulations and ethics.

Exploring the ethical aspects of AI is just as important as the technological progress it brings. As an entrepreneur who has seen the impact of AI, I believe it's essential to balance innovation with responsibility. This is not only a moral obligation but also a strategic requirement for the long-term success and sustainability of businesses in the epoch of Industry 4.0.",,
47,"In the rush to adopt AI, ethics and responsibility are taking a backseat at many companies",https://www.businessinsider.com/ai-ethics-risks-responsibility-business-bcg-2024-5,Dan,News Article,"In the rush to adopt AI, ethics and responsibility are taking a backseat at many companies

AI can get pretty scary if its not regulated. South_agency/Getty Images
Companies are rapidly integrating generative AI technology to boost productivity.
Experts, however, are concerned that efforts to manage the risks of AI are lagging.
Responsible AI efforts are moving ""nowhere near as fast as they should be,"" a BCG senior partner said.
Companies have been racing to deploy generative AI technology into their work since the launch of ChatGPT in 2022. 

Executives say they're excited about how AI boosts productivity, analyzes data, and cuts down on busy work.

According to Microsoft and LinkedIn's 2024 Work Trends report, which surveyed 31,000 full-time workers between February and March, close to four in five business leaders believe their company needs to adopt the technology to stay competitive.

But adopting AI in the workplace also presents risks, including reputational, financial, and legal harm. The challenge of combating them is that they're ambiguous, and many companies are still trying to understand how to identify and measure them.

AI programs run responsibly should include strategies for governance, data privacy, ethics, and trust and safety, but experts who study risk say the programs haven't kept up with innovation.

Efforts to use AI responsibly in the workplace are moving ""nowhere near as fast as they should be,"" Tad Roselund, a managing director and senior partner at Boston Consulting Group, told Business Insider. These programs often require a considerable amount of investment and a minimum of two years to implement, according to BCG.

Related stories

VCs clamor to invest in the red-hot early-stage round for AI safety startup Haize Labs


This might be the most important job in AI

That's a big investment and time commitment and company leaders seem more focused instead on allocating resources to quickly develop AI in a way that boosts productivity.

""Establishing good risk management capabilities requires significant resources and expertise, which not all companies can afford or have available to them today,"" researcher and policy analyst Nanjira Sam told MIT Sloan Management Review. She added that the ""demand for AI governance and risk experts is outpacing the supply."" 

Investors need to play a more critical role in funding the tools and resources for these programs, according to Navrina Singh, the founder of Credo AI, a governance platform that helps companies comply with AI regulations. Funding for generative AI startups hit $25.2 billion in 2023, according to a report from Stanford's Institute for Human-Centered Artificial Intelligence, but it's unclear how much went to companies that focus on responsible AI.

""The venture capital environment also reflects a disproportionate focus on AI innovation over AI governance,"" Singh told Business Insider by email. ""To adopt AI at scale and speed responsibly, equal emphasis must be placed on ethical frameworks, infrastructure, and tooling to ensure sustainable and responsible AI integration across all sectors.""

Legislative efforts have been underway to fill that gap. In March, the EU approved the Artificial Intelligence Act, which assigns the risks of AI applications into three categories and bans those with unacceptable risks. Meanwhile, the Biden Administration signed a sweeping executive order in October demanding greater transparency from major tech companies developing artificial intelligence models. 

But with the pace of innovation in AI, government regulations may not be enough right now to ensure companies are protecting themselves.

""We risk a substantial responsibility deficit that could halt AI initiatives before they reach production, or worse, lead to failures that result in unintended societal risks, reputational damage, and regulatory complications if made into production,"" Singh said.",,
52,AI Generates Debate Over Newsroom Ethics,https://www.voanews.com/a/ai-generates-debate-over-newsroom-ethics-/7450743.html,Awais,News Article,"With the growth of advanced artificial intelligence and its ability to help spread mis- and disinformation, some media experts believe the journalism industry should adopt uniform standards on the new technology.

Among the questions discussed by newsroom leaders is how AI — which has a record of errors and so-called digital hallucinations, and is already used to create deepfakes — can be ethically relied on by an industry whose credibility depends on trust.

“I think it will take some time for news organizations to develop best practices,” said Jared Schroeder, an associate professor specializing in media law and technology at the University of Missouri School of Journalism.

“There is no set best practices yet and we have two problems: It’s new and it’s changing. We are not done. The AI of today will be different next year and in five years,” he added.

Generative AI poses an interesting dilemma for an industry suffering economic decline. The technology can assist in producing transcripts, editing copy, narrating audio or TV packages and creating images. And investigative news outlets have long relied on it to trawl large data sets.


How AI Could Act as Boost for Investigative Journalism
But it also presents a risk for copyright violations and plagiarism, along with errors.

The New York Times last month sued Open AI and Microsoft for copyright infringement. And an August report by the watchdog NewsGuard found AI chatbots plagiarizing thousands of news articles.

As news organizations start adopting standards and practices, many experts agree that AI is a useful tool for journalism, but that people are still needed to oversee its application.

“As a journalist I am not allowed to use AI to draft my stories or anything like that,” said Ryan Heath, the global technology correspondent for the news website Axios. “It is fine to use it to do a bit of research and prompt you for inspiration, but you cannot use it to do the actual reporting or drafting of your articles,” he told VOA.

News outlets experimenting with using AI in the place of reporters have done so with only limited success.

The U.S. media outlet Sports Illustrated in November was accused of publishing AI-generated content under fake bylines. Sports Illustrated denied the allegations, saying a third party provided the content. It fired senior executives the following month, but denied its decision was connected to the AI allegations.

The outlet was back in the news Friday, announcing mass staff layoffs after losing its licensing agreement in relation to missed payments.

Separately, the tech website CNET’s experiment using AI to assist in producing stories early last year resulted in dozens of articles containing errors. CNET published at least 41 corrections, according to the Verge. The site's then editor-in-chief said in a statement at the time, ""We've paused and will restart using the AI tool when we feel confident the tool and our editorial processes will prevent both human and AI errors.""

Heath of Axios said his media outlet had adopted a more cautious approach.

“They recognize that it is definitely a big transformation, so they hired people like me to write full time about AI,” he said. “[But] they want to stop and think about it first.”


Deepfakes a 'Weapon Against Journalism,' Analyst Says
Axios is not alone in hiring staff to focus on AI. The New York Times last month announced Zach Seward as editorial director of AI initiatives.

In a press release, the publisher said it would be Seward’s job to establish principles for using artificial intelligence at the organization. VOA reached out to the Times to request an interview, but the paper declined.

Others, like The Associated Press, are signing licensing agreements with Open AI, ChatGPT’s maker.

AP also declined VOA’s request for interviews. But in press releases the news agency has said: “Accuracy, fairness and speed are the guiding values for AP’s news report, and we believe the mindful use of artificial intelligence can serve these values and over time improve how we work.”

The implications of AI on newsrooms are a key trend for 2024, especially in a year when more than 40 countries will hold significant elections.

“Embracing the best of AI while managing its risks will be the underlying narrative of the year ahead,” wrote Nic Newman, senior research associate at the Reuters Institute for the Study of Journalism in the organization’s annual media trends report.

Noting that questions on trust and intellectual property are key, Newman added, “Publishers can also see advantages in making their businesses more efficient and more relevant for audiences.”

Newsroom leaders and media watchdogs are weighing in, too.

Nobel laureate Maria Ressa joined with Reporters Without Borders and other groups to release the Paris Charter on AI and Journalism, in November. The charter’s creators say they want it to serve as an ethics blueprint for AI use in journalism and want news organizations to adopt its 10 principles.

But so far, the charter hasn’t been adopted by many news organizations. And the list of journalists using AI is growing.

Pandora’s box has been opened, said Schroeder, adding, “It would be dangerous for journalism to not be thinking about how AI should be used. It doesn’t mean every news organization should use it the same way.”

Some governments seem to share that view.

A U.S. Senate Committee on the Judiciary subcommittee hearing on January 10 looked at reservations about how the technology could impact journalism.

“It is in fact a perfect storm of declining revenue and exploding disinformation, and a lot of the cause of this perfect storm is in fact technologies like artificial intelligence,” said Senator Richard Blumenthal, a Democrat from Connecticut, who chaired the hearing.

And in December the European Union passed the Artificial Intelligence Act, to ensure safe and transparent use of the technology. Included are requirements for tech companies to disclose when content is generated by AI.

Media experts are also emphasizing transparency and human oversight in their discussions of how and when AI is used in journalism.",,
61,Davos 2024: Can – and should – leaders aim to regulate AI directly?,https://www.bbc.com/worklife/article/20240118-davos-2024-can-and-should-leaders-aim-to-regulate-ai-directly,Hiruni,News Article,"Davos 2024: Can – and should – leaders aim to regulate AI directly?

Andrew Ng, founder of DeepLearning.AI, believes regulating the development of artificial intelligence could stifle innovation (Credit: Getty Images)
At this year's World Economic Forum in Davos, global leaders are asking whether to put guardrails in place for AI itself, or look to regulate its effects once the tech is developed.

Artificial intelligence (AI) is top of mind for many workers who are hopeful about its possibilities, but wary of its future implications.
Earlier this month, researchers for the International Monetary Fund found that AI may affect the work of four in 10 employees worldwide. That number jumps to six in 10 in advanced economies, in industries as diverse as telemarketing and law. Additionally, a just-released report by the World Economic Forum showed half of the economists surveyed believed AI would become ""commercially disruptive"" in 2024 – up from 42% in 2023.
Business leaders at 2024's World Economic Forum's Annual Meeting in Davos are also prioritising conversations about artificial intelligence. Particularly, they're focused on how to regulate AI tech to make sure it's a force for good in both business and the world at large.
""This is the most powerful technology of our times,"" Arati Prabhakar, director of the US White House Office of Science and Technology Policy, said at a Davos panel on 17 January. ""We see the power of AI as something that must be managed. The risks have to be managed.""
The conversation around how, exactly, governance can help manage that risk comes at a crucial inflection point. Alongside AI's emerging benefits has been a more complicated and darker reality: artificial intelligence can have unintended consequences and even nefarious uses. Take, for example, the software that automatically (and unlawfully) turned down job applicants over a certain age, or the AI-powered chatbot that spouted sexist and racist tweets.
Governments in regions including the EU and US have been discussing AI regulations for years – yet there are still almost as many questions as answers. One of the stickiest is around how to regulate AI, exactly – and to what extent it even can be regulated.
Getty Images In AI-assisted healthcare, the effects of the technology are already being governed in some countries (Credit: Getty Images)Getty Images
In AI-assisted healthcare, the effects of the technology are already being governed in some countries (Credit: Getty Images)
One approach on the table at Davos is regulating the way artificial intelligence works from the start.
This could mean putting in place policies that evaluate and audit algorithms. The core idea is to ensure the algorithms themselves aren't misusing data in ways that could lead to unlawful outcomes. In the US, for example, federal agencies have proposed that quality-control assessments be established for algorithms that evaluate a property's collateral value in mortgage applications. 
Some experts in the industry, however, are concerned that restrictive regulation of the technology at its origin could stifle innovation and development.
As Andrew Ng, founder of the AI education company Deeplearning.AI and an adjunct computer science professor at Stanford University, put it in a recent interview with the World Economic Forum, ""Regulations that impose burdensome requirements on open-source software or on AI technology development can hamper innovation, have anti-competitive effects that favour big tech, and slow our ability to bring benefits to everyone.""
Other leaders think it may be impractical to even attempt to directly govern artificial intelligence technology in the first place. Instead, they argue the regulation should take aim at the effects of AI once it's developed. 
""Artificial intelligence by the name is not something that you can actually govern,"" Khalfan Belhoul, chief executive officer of the Dubai Future Foundation, told the World Economic Forum on a recent episode of Radio Davos. ""You can govern the sub-effects or the sectors that artificial intelligence can affect. And if you take them on a case-by-case basis, this is the best way to actually create some kind of a policy.""
In some ways, this approach is already in action.
""There are a wide variety of laws in place around the world that were not necessarily written for AI, but they absolutely apply to AI. Privacy laws. Cybersecurity rules. Digital safety. Child protection. Consumer protection. Competition law,"" Brad Smith, vice-chair and president of Microsoft, said at Davos. ""And now you have a new set of AI-specific rules and laws. And I do think there's more similarity than most people assume.""
But others say there should be some standard set of rules by which to regulate AI technologies specifically.
These kinds of things are still very nascent. No one has answers just yet – Josephine Teo
Indeed, points out Wendell Wallach, senior scholar at the Carnegie Council for Ethics in International Affairs, where he is co-director of the AI and Equality Initiative, and author of the book Dangerous Master: How to keep technology from slipping beyond our control, some industries already have a number of regulations in place that also should regulate AI. 
In terms of governing outcomes, regulations already exist – in some industries more than others. ""Healthcare is pretty well regulated already, at least in the United States and Europe,"" says Wallach. ""In a sense, many of the AI applications are going to be regulated by what already exists. So, the question is, what additional [regulations] do you need? You probably need different forms of testing and compliance in some areas.""
For example, in the US, healthcare regulations hold that a physician has a duty of care; if they misdiagnose a patient, they could be liable for medical malpractice. If a doctor relies on AI to help diagnose patients and gets a diagnosis wrong, they could still be liable (although, experts note, there's the issue of plausible deniability: whether it will become common to blame the AI for making the mistake). But that means governing the outcome – the misdiagnosis.
And many experts think even that doesn't go far enough. There need to be more safety measures in place to regulate the processes itself: quality-control assessments of the underlying data that made that misdiagnosis, for example.
""There will have to be different ways of demonstrating whether an AI is being implemented in a responsible way. And the question of how do you implement tests? How do you benchmark them?"" said Josephine Teo, Singapore minister for communications and information, at a Davos panel. ""These kinds of things are still very nascent. No one has answers just yet.""
Overall, experts say, the key goal is to ensure that AI is used for good – whatever the approach.
""If you think about the work that's ahead of us – to deal with the climate crisis, to lift people's health and welfare, to make sure our kids get educated, and that people in their working lives can train for new skills… it's hard to see how we're going to do them without the power of AI,"" said Prabhakar. ""In the American approach, we've always thought about doing this work of regulation as a means to that end – not just to protect rights, which is completely necessary not only to protect national security, but also to achieve these great aspirations.""",,
67,AI Ethics: 7 Crucial Qualities Of Ethical Leadership,https://www.forbes.com/sites/bruceweinstein/2024/02/21/ai-7-crucial-qualities-of-ethical-leadership/,Awais,News Article,"AI Ethics: 7 Crucial Qualities Of Ethical Leadership
Bruce Weinstein, Ph.D.
AI ethics keynotes globally. ""Best ethics talk I've ever attended.""

AI ethics or AI Law concept. Developing AI codes of ethics. Compliance, regulation, standard , business policy and responsibility for guarding against unintended bias in machine learning algorithms.
AI is cool. Using AI with ethical intelligence is even cooler!GETTY
How can you work with artificial intelligence the right way? The best leaders consistently apply these crucial qualities of ethical leadership:

1. Honesty

2. Accountability

3. Care

4. Courage

5. Fairness

6. Gratitude

7. Humility

Let’s take a closer look at each one. I will define each quality and present leaders who exemplify each of these traits in their work with AI.

1. Honesty

By signing up, you agree to our Terms of Service, and you acknowledge our Privacy Statement. Forbes is protected by reCAPTCHA, and the Google Privacy Policy and Terms of Service apply.
The most fundamental character trait of all is honesty. It doesn’t matter how caring or grateful a person is; if they’re not honest, there’s no reason to trust them.

Honesty is, above all, a feeling, a disposition, an orientation toward the truth. Honest leaders cannot tolerate lying, fudging data, misrepresenting themselves or their companies, or engaging in any other conduct that displays contempt for the truth. Falsehood in all its forms is poison to an honest person.

One way that leaders demonstrate their commitment to honesty is through transparency. With respect to AI, this involves openly sharing how AI systems make decisions, explaining what data are being used, and acknowledging the limitations and biases of these systems. By doing so, such leaders build trust and accountability in their use of AI.

But a study by Stanford HAI (Human-Centered Artificial Intelligence) found that AI models often lack transparency. The Foundation Model Transparency Index ranked 10 major AI firms, including Meta’s Llama 2, OpenAI’s GPT-4, and Google’s PaLM 2 and found them woefully lacking in this aspect of honesty. Amazon had only 12% transparency with respect to its Titan Text. “No major foundation model developer is close to providing adequate transparency, revealing a fundamental lack of transparency in the AI industry,” the study found.

Why should these firms and the public be concerned? A lack of transparency also affects the ability of businesses to rely on the technology and compromises the ability of consumers to understand the models' limitations.

Whoever said “There’s no such thing as bad publicity” didn’t have a report like this in mind. (The proverb is often attributed to P.T. Barnum, but whether he actually said it is not clear.)

2. ​​Accountability
IT STARTS WITH YOU. words. text on grey paper on torn paper background
The word ""accountability"" may start with an ""a,"" but the concept of accountability does indeed start
Accountable leaders do four things consistently:

· Keep their promises

· Consider the consequences of their actions

· Take responsibility for their mistakes

· Make amends for their mistakes

Note my use of the word “consistently.” It’s not enough to keep your promises once in a while or to take responsibility for your mistakes here and there. Ethical leadership in general and accountable leadership in particular means doing these four things day in and day out. Yes, we all fall short occasionally, but true accountability means hitting the mark more often than not.

Amazon once used an AI recruitment tool that turned out to be biased against female applicants. It was trained on resumes submitted over a decade, mostly by men, and the AI learned to prefer male candidates over female candidates. To Amazon’s credit, the company stopped using the system after attempts failed to correct the problem. This is a great example of a company taking responsibility for its mistakes and then making the proper amends.

3. Care
Smiling female doctor standing with medical colleagues in a hospital
Caring people are cool.
Care, like honesty, is first and foremost a strong feeling or passion. Where honesty means having a passion for truth, care means having a deep concern for people’s well-being and flourishing. Many people include animals and the environment among the things they care about. Of course, caring people do more than feel strongly about helping people. They put those feelings into action.

Home Helpers, a home care service provider, evinces a caring approach in its use of artificial intelligence. The company's President and CEO, Emma Dickison, has discussed how the business is exploring AI applications for improved home-monitoring technology, virtual assistants, and marketing solutions. Dickison emphasizes that any AI solution must be cost-effective and respectful of privacy. A company committed to ethical leadership rightly wants to avoid being an Orwellian “Big Brother.”

Home Helpers is cautious with AI tools. It avoids using free versions of tools like ChatGPT that might compromise proprietary information. Dickson told Home Health Care News writer Joyce Famakinwa:

“We don’t allow anyone, internally, to use the free version of ChatGPT because that feeds into their algorithms. It can give away your IP, your proprietary information, etc. We make sure that we use a premium version. You just want to be really careful and put guardrails up, and internal policies that recognize how and when you can use AI in your business. What gives you the best reward and assumes the least risk.”

It is fitting that a company devoted to home care exemplifies a caring approach to every aspect of its mission. “[W]e will continue to work on behalf of our clients to keep care affordable and for our caregivers to be able to have a professional career long-term in this industry,” Dickison told me in an interview.

4. Courage
Woman jumping over abyss.
The best leaders are courageous leaders.GETTY
""Courage is resistance to fear, mastery of fear, and not the absence of fear,"" wrote Mark Twain. The field of AI is a rich place to see courageous leadership in action.

Consider, for example, Dr. Timnit Gebru, a former co-lead of the Ethical Artificial Intelligence team at Google. Her groundbreaking research, conducted with the Algorithmic Justice League founder Joy Buolamwini, exposed biases in AI systems, particularly in facial recognition software, towards people of color. This research rightly prompted significant discussions and moratoriums on the use of such software.

In an article about Dr. Gebru for MIT Technology Review, Karen Hao wrote, ""In the immediate aftermath [of her dismissal from Google], over 2,600 Google employees and 4,300 others signed a petition denouncing Gebru’s dismissal as 'unprecedented research censorship.' Half a year later,” Hao continued, “research groups are still rejecting the company’s funding, researchers refuse to participate in its conference workshops, and employees are leaving in protest.""​

Dr. Gebru is a shining example of a courageous leader, because she has consistently challenged major tech companies on their ethical AI practices. She has risked her professional standing to advocate for accountability and highlight inherent biases in AI systems, particularly against marginalized groups.

How far would you go to stand up for what you believe in?

5. Fairness
Businessman touching the icon of the balance of justice. Concept of legal advice, law and defense.
What does fairness in the context of AI leadership mean? Read on.GETTY
To be fair is to give to others their due. We sometimes speak of fairness in terms of justice. For example, in the workplace, fairness means ensuring that employees are paid what they’re worth (economic justice), are free from harassment (social justice), and are given an opportunity to have their grievances and disputes addressed (procedural justice).

Here's an example of how one company took its commitment to fairness seriously with respect to AI. Fujitsu Laboratories formed an international AI Ethics Research Team, comprising members from Japan, the U.S., and Europe, to tackle the ethical challenges in AI development. Bias in data and algorithms are a pernicious and pervasive problem in AI systems, as we’ve discussed. Without preventive measures in place, such bias can lead to unethical outcomes such as racial and gender discrimination.

“To avoid discrimination in AI decisions, we should first prevent unfair biases from being included in AI models and datasets,” notes Yuri Nakao, a member of the team who is based in Japan.

6. Gratitude and 7. Humility
speech bubbles with text thanks in different languages
Both gratitude and humility are bona fide ethical issues, because they are both matters of ...
Gratitude and humility are two sides of the same coin. Humble leaders recognize that they can’t achieve greatness all by themselves. It takes a team to do it. Grateful leaders regularly acknowledge the work that team members do to make greatness possible.

Satya Nadella's tenure as CEO of Microsoft is a prime example of how gratitude and humility can profoundly influence leadership for the better with respect to AI. When he moved up to the leadership role in 2014, Nadella shifted Microsoft's culture from aggressive competition to one emphasizing empathy, collaboration, and ethical responsibility. By focusing on augmenting human capabilities rather than replacing them, he steered the company towards a more humane approach to AI. There were two positive outcomes: Nadella’s approach not only democratized AI but also resulted in significant financial success for Microsoft.

Yes, it is possible to do well by doing good. Satya Nadella is an inspiring example of this.

Summary
RECAP - word is written on wooden cubes on a bright blue background. close-up of wooden elements
Here's the essence of the article, distilled to a few words.GETTY
The best leaders in the AI sector consistently exemplify these character traits:

1. Honesty

2. Accountability

3. Care

4. Courage

5. Fairness

6. Gratitude

7. Humility

We all mistakes from time to time in each of the above areas. But ethical leadership means doing our level best to exemplify these traits more often than not. The people we serve deserve nothing less.",,
70,AI could transform ethics committees,https://theconversation.com/ai-could-transform-ethics-committees-224424,Rhea,Research Paper,"AI could transform ethics committees
The role of an ethics committee is to give advice on what should be done in often contentious situations. They are used in medicine, research, business, law and a variety of other areas.

The word “ethics” relates to the moral principles governing human behaviour. The task for ethics committees can be quite tricky given the wide range of moral, political, philosophical, cultural and religious views. Even so, good ethical arguments make up the foundation of society, as they are the basis of the laws and agreements that we use to get on with each other.

Given the importance of ethics, any tool that can be used to help come to better ethical decisions should be explored and used. Over the last couple of years, there has been an increasing recognition that artificial intelligence (AI) is a tool that can be used to analyse complex data. So it makes sense to ask the question of whether AI can be used to help make better ethics decisions.

As AI is a class of computer algorithm, it relies on data. Ethics committees also rely on data, so one important question is whether AI is able to load, and then meaningfully analyse, the types of data that ethics committees regularly consider.

Start your day with evidence-based news.
Here, context becomes very important. For instance a hospital ethics committee might make decisions based upon experience with patients, input from lawyers, and a general understanding of common cultural or societal norms and opinions. It is currently difficult to see how such data could be captured, and fed into, an AI algorithm.

However, I chair a very specific type of ethics committee, called a research ethics committee (REC), whose role is to review scientific research protocols. The aim is to promote high quality research while protecting the rights, safety, dignity and well being of the people who take part in the research.

Laboratory setting.
Ethics committees review scientific research involving human subjects. 
The majority of our activity involves reading complex documents to determine what the relevant ethics issues may be, and then making suggestions to researchers on how they can improve their proposed protocols, or procedures. It is in this area that AI could be very helpful.

Research protocols, especially those of clinical trials, often run to hundreds if not thousands of pages. The information is dense and complex. Although protocols are accompanied by ethics application forms that seek to present information on key ethics issues in a way that REC members can easily find, the task can still take a very long time.

After studying the documents, REC members weigh up what they have read, compare it with guidance on good ethics practice, consider input from patient and participant involvement groups, and then come to a decision as to whether the research can proceed as planned. The most common outcome is that more information and a few modifications are needed before the research can go ahead.

A role for machines?
While attempts have been made to standardise REC membership and experience, researchers often complain that the process can take a long time and is inconsistent between different committees.

AI seems ideally placed to speed up the process and assist in ironing out some of the inconsistencies. Not only could the AI read such long documents very quickly, but it could also be trained on a large number of previous protocols and decisions.

It could very rapidly spot any ethics issues and suggest solutions for the research teams to implement. This would vastly speed up the ethics review process and probably make it far more consistent. But is it ethically acceptable to use AI in this way?

While AI could clearly conduct many of the REC tasks, it could also be argued that these reviewing tasks are not actually the same as making an ethics decision. At the end of the review process, RECs are asked to decide whether a protocol, with the updates, should receive a favourable or unfavourable opinion.

As a consequence, while the advantage of AI is clear in speeding up the process, this isn’t quite the same as making the final decision.

A human in the loop
It may be possible for AI to be extremely effective in assessing a situation and recommending a course of action that is consistent with previous “ethical” behaviour. However, the decision to actually adopt a course of action, and then go on to behave in that way, is fundamentally human.

In the example of research ethics, the AI might well recommend a course of action, but actually deciding on the action is a human decision. The system could be designed to instruct ethics committees or researchers to unquestionably do what the AI suggests, but such a decision is about how the AI is used, not the AI itself.

While AI is perhaps immediately useful to research ethics committees given the type of data we review, it is very likely that ways of encoding non-text data (such as people’s experiences) will improve.

This means that over time AI may also be able to assist in other areas of ethics decision making. However, the key point is not to confuse the tool used to analyse data, the AI, with the final “ethics” decision on how to act. The danger is not the AI, but how people choose to integrate AI into ethics decision making processes.",,
71,"How Ethics, Regulations And Guidelines Can Shape Responsible AI",https://www.forbes.com/sites/forbestechcouncil/2024/02/05/how-ethics-regulations-and-guidelines-can-shape-responsible-ai/,Awais,News Article,"How Ethics, Regulations And Guidelines Can Shape Responsible AI Ethics plays a crucial role in guiding the development of artificial intelligence (AI), as does outlining fundamental principles such as fairness, transparency and accountability. However, principles alone may not be sufficient, and the challenge lies in bringing clarity to these ethical foundations.

In light of AI's transformative shifts and potential biases, technology companies must lead by example. This involves adopting stringent ethical guidelines and actively creating global standards and regulatory frameworks. The tech industry must prioritize legal certainty and a comprehensive and inclusive approach that protects human rights in diverse cultural contexts.

In this article, I'll look at the current evolution of AI guidelines and then explain how the technology sector can play a bigger role in their development and implementation.

Strides Toward Ethical AI
Several insightful ideas exist regarding what this clarity could entail, starting with a three-pillar approach to AI governance proposed by Telefonica. This approach—encompassing global guidelines, self-regulation and a regulatory framework—forms a robust structure to ensure that AI aligns closely with the world's best interests.

World leaders have also made strides toward establishing AI guidelines. For example, the Bletchley Declaration, crafted in early November through consensus among 29 countries, including Germany, the United States and China, marks a significant step forward in shaping responsible AI development. The declaration emphasizes the global opportunities presented by AI, highlighting its potential to enhance human well-being, peace and prosperity.

It also underscores the need for safe, human-centric, trustworthy and responsible AI development and usage. The document recognizes the increasing reliance on AI in various sectors, such as healthcare, education and justice, emphasizing the importance of safe development and its inclusive and beneficial global use.

Additionally, the Bletchley Declaration addresses the risks and challenges associated with AI, particularly in daily life domains, frontier AI risks and international cooperation to tackle AI-related issues. The declaration outlines the roles of various actors, places responsibility on developers for AI safety and advocates for sustained global dialogue, research and responsible harnessing of AI benefits.

Overall, the Bletchley Declaration strives to balance harnessing AI's potential and mitigating its risks globally.

Parallel to these efforts, UNESCO's recommendations on AI ethics echo the call for a cohesive global framework, aiming to create consistency in standards across diverse regions and cultures.

These collective efforts steer the narrative toward a future where AI innovates with integrity and a steadfast commitment to ethical principles.

AI's Impact On Society And Individuals
Despite the progressive nature of these ideas, there is room for improvement. Presently, society and individuals remain divided over the role of AI in their lives, with concerns about privacy, surveillance and the potential for discriminatory outcomes positioning AI innovations negatively.

With its inherent capacity to learn, AI lacks a moral center unless developed with one, making it susceptible to bias and discrimination. These negative traits can perpetuate or amplify societal inequalities and significantly infringe on human rights if not addressed. Economies are on the brink of a transformative shift, with AI poised to replace countless jobs. This impending reality requires urgent attention and promoting responsible innovation emerges as a potential solution.

We must, therefore, balance AI advancements, ethical norms and societal values to create an environment where ethical AI research and development are encouraged and supported. UNESCO, for example, recognizes the impact of AI on economies and labor, emphasizing the need for a range of skills in education to prepare for changing job markets.

The Great Balancing Act
AI's potential misuse is a serious threat that will continue to evolve. Equally pressing is the rate at which we can develop and apply comprehensive frameworks for ethics and regulation. Generative AI is swiftly advancing, and achieving consensus requires all stakeholders to collaboratively develop suitable regulations that ensure a future characterized by responsible AI.

Balancing responsible AI with innovation in the technology industry is crucial to ensure both ethical practices and technological advancements. Here are some specific steps that companies can take internally to achieve this balance:

1. Establish ethical AI principles prioritizing fairness, transparency, accountability and privacy. These principles should guide all AI-related decisions.

2. Build and nurture a diverse and inclusive workforce that brings together individuals with varied backgrounds and perspectives. This diversity can help identify and address potential biases in AI algorithms.

3. Implement tools and processes for detecting and mitigating biases in AI algorithms. Regularly audit and update algorithms to ensure they do not discriminate against certain demographic groups.

4. Prioritize user-centric design that respects user privacy and preferences. Communicate to users how their data will be used and provide options for users to control and manage their personal information.

5. Develop flexible and adaptive policies to changing ethical standards and technological advancements. Regularly review and update these policies to align with the evolving landscape.

6. Adopt a principle of data minimization, collecting only the necessary information and discarding unnecessary data. This reduces the risk of misuse and enhances user privacy.

7. Work closely with regulatory bodies and industry organizations to stay informed about evolving standards and regulations related to identity verification and AI. Actively participate in discussions and contribute to the development of responsible AI guidelines.

By integrating these steps into their internal processes, companies can foster responsible AI innovation while addressing ethical considerations and building trust with users and regulatory authorities.
",,
89,AI will be everywhere. How should we prepare for the ethics around this?,https://www.thenationalnews.com/opinion/comment/2024/05/17/ai-ethics-google-generative/,Dan,News Article,"AI will be everywhere. How should we prepare for the ethics around this?

This week, senior officials from the world’s two largest economies have been in talks about how to manage a future dominated by artificial intelligence. It should be taken as a positive sign that the US and China are able to get together for discussions in Geneva, but it is also worth making the point that while we wait for governments and corporations to set the rules of engagement, there is much we can do as people to prepare ourselves for the inevitable moment when AI is all-pervasive.

Ask yourself what you think about how AI should be used. What are your own values when it comes to privacy, fairness and the potential environmental impact from the myriad applications of AI? As people and professionals, we have a small window during which we can begin to decide for ourselves the boundaries of AI for our lives.


We have the past decade as a frame of reference, when the benefits and costs of keeping a digital device in our hands at all times have become clearer. It affects our well-being and mental health, as well as enables us to fulfil our dreams just like in the old tales of jinn and magic lamps. The moral of those stories is still very much relevant today; such power must never be taken lightly.

At Google’s invitation, I spent a few fascinating days at the company’s Zeitgeist conference, held just outside London. There it became very clear to me, after listening to experts from both within its ranks and beyond, that we are at the start of an era that will result in AI becoming central to our lives. It will affect how we travel, receive health care, keep ourselves safe, plan the communities in which we live and communicate with each other. There is no going back now.

The ethical discussions about how and when AI should be used are decades old. Engineers and experts working in the field have grappled with the issues, but the answers will need to come from a broader cross-section of society if we are to plot a positive path forward.

The emergence of Generative AI technology has accelerated and expanded the scope of the debate.

According to UK government data, “a third of the public report using chatbots at least once a month in their day-to-day lives. In parallel, self-reported awareness and understanding of AI has increased across society, including among older people, people belonging to lower socio-economic grades and people with lower digital familiarity”.

Self-reported awareness and understanding of AI has increased across society, including among older people
However, the same report suggests that “alongside this increased understanding, there are ongoing anxieties linked to AI. A growing proportion of the public think that AI will have a net negative impact on society, with words such as ‘scary’, ‘worry’ and ‘unsure’ commonly used to express feelings associated with it”.

In the US, Pew Research Centre surveys showed similar findings with almost all Americans aware of the growing role of AI and a majority feeling concerned about where the technology is taking us. In essence, as the understanding grows, so will the need to ensure that public discourse is based on fact and not on fear mongering – either for or against the use of AI.

For example, while AI’s potential to make misleading and fake information harder to spot is being flagged repeatedly, little has been said about the risk of disinformation about AI technology itself to society. A vacuum in terms of education about the truth of AI allows bad actors to exploit the general feelings of anxiety related to the rise of this technology. There is the risk that such malign actions will further polarise and destabilise communities.

More from Mustafa Alrawi
Has extreme weather altered people's attitude towards climate change?

We need, therefore, to discuss it as much as possible and in an open and frank way that allows the public to become more informed about both the pros and the cons.

“Prebunking”, as Google defines campaigns to help people identify and resist manipulative content in advance of misinformation being put in the public domain, will be increasingly necessary. But who should be responsible for this? A mix of government and corporate entities? A better question might be to ask how people can take control of their destiny and educate themselves about the realities of AI rather than wait for governments and companies to choose for them.

During the Covid-19 pandemic, for example, while we of course adhered to public safety rules and regulations, we each had to navigate the day-to-day of what it meant to stay safe, how to balance physical and mental health, and make decisions such as which vaccine to take and when. It was very stressful, to say the least, and moreover we had to do it in a compressed period due to the nature of the crisis.

At the moment, when it comes to AI, we have the relative luxury of time, but what is at stake is no less serious.

Using AI can enhance the good that we do, but it could also further deepen the negative aspects of society. It is not just a matter of regulation. It is an ethical and moral subject as much as a practical one.",,
90,Address ethical concerns to optimise AI use,https://www.tribuneindia.com/news/comment/address-ethical-concerns-to-optimise-ai-use-640753,Awais,News Article,"Address ethical concerns to optimise AI use
India’s unique demographic and socioeconomic context provides a fertile ground for AI applications. INDIA is at a pivotal juncture in its technological evolution, with artificial intelligence (AI) rapidly advancing in sectors like healthcare, agriculture, education and finance. Government initiatives such as the National Strategy for AI (NSAI) and NITI Aayog’s efforts highlight India’s ambition to be a global AI leader. The ‘AI For All’ initiative aims to democratise AI, ensuring inclusive growth and accessibility for all citizens. Collaborative efforts between government bodies and private sector companies are fostering a robust environment for AI innovation and application.

A robust policy framework is crucial for sustainable AI development. The Personal Data Protection Bill, 2019, emphasises safeguarding citizens’ data, addressing surveillance, data misuse and ethical AI use. Given AI systems’ handling of vast personal data, stringent data protection laws are central to ensuring secure data handling and prevent breaches. Ethical AI development must align with guidelines promoting fairness, transparency, accountability and bias avoidance. Regulatory sandboxes can facilitate supervised testing of AI solutions by startups and researchers, encouraging innovation while upholding ethical standards. Addressing algorithmic bias is essential to ensuring fairness and inclusivity in AI applications. Frameworks for ethical AI use that promote fairness, accountability and transparency can build public trust. It involves educating people about AI’s benefits and potential risks.

India faces challenges pertaining to technological infrastructure, data availability and a skilled workforce. These challenges present opportunities for upskilling and education initiatives to build AI expertise. Investment in AI infrastructure, like data centres, cloud computing and high-speed Internet, is crucial. Encouraging AI research through grants, scholarships and collaborations between academia and industry can drive innovation. Building public trust in AI involves transparent communication about its benefits and risks and addressing fears of job displacement due to AI automation. Global collaborations, learning from the EU’s AI strategy and OECD AI Principles enhance India’s AI policies. Leveraging AI boosts India’s global tech competitiveness. A balanced regulatory approach fosters innovation aligned with public interest and ethics. Incentives for AI research, development and startup investments nurture a vibrant innovation ecosystem, supporting AI-driven startups and SMEs (small and medium enterprises).

AI can tackle major societal challenges. In healthcare, it enables early diagnosis, personalised treatment and efficient delivery, improving outcomes. In education, AI-driven tools offer personalised learning and adaptive technologies, revolutionising the landscape. For environmental sustainability, AI helps monitor resources, predict natural disasters and mitigate the impacts of climate change, optimising energy, agriculture and waste management for more sustainable practices.

In agriculture, AI-driven precision farming boosts productivity, optimises resources and supports farmers with predictive analytics. In healthcare, AI improves diagnostics and treatment recommendations. Public-private partnerships with tech giants like Google, Microsoft and IBM foster AI research, training and capacity-building, keeping India at the forefront of AI innovation.

India is actively advancing AI through initiatives like the NSAI by NITI Aayog, promoting AI research, development and deployment in healthcare, agriculture, education, smart cities and infrastructure. The ‘AI For All’ initiative aims to democratise AI, enhancing literacy and capacity-building. Practical applications, such as the Aarogya Setu for Covid-19 contact tracing and health updates, highlight AI’s real-world impact.

A balanced AI policy involves creating a regulatory environment that fosters innovation while protecting the public interest. Developing and enforcing ethical standards for AI development and deployment are crucial. Ensuring AI systems are designed with fairness, accountability and transparency is essential to building public trust. Comprehensive data protection laws are critical to safeguarding personal information and promoting best practices for data security and privacy, preventing data breaches and misuse and ensuring responsible AI use.

Inclusive and participatory policy-making is essential for effective AI policies. India’s AI landscape is grounded in substantial data and real-world examples, highlighting its transformative potential. For instance, AI-driven initiatives like the Ayushman Bharat Digital Mission aim to integrate digital health infrastructure with AI capabilities to improve patient outcomes and streamline healthcare services. Accenture predicts that AI could add $957 billion to India’s economy by 2035, underscoring the economic imperative of robust AI integration.

India’s unique demographic and socioeconomic context provides a fertile ground for AI applications. The Indian Council of Agricultural Research collaborates with IBM to use AI for crop yield prediction, pest control and weather forecasting, significantly aiding farmers in the decision-making process. AI-driven precision farming techniques, as per the Ministry of Agriculture, can increase crop yields by up to 30 per cent, showcasing the tangible benefits of AI adoption in this sector.

In the field of education, AI-powered platforms like Vedantu have revolutionised learning by providing personalised education experiences. These platforms leverage AI algorithms to tailor educational content to individual learning styles and paces, improving student engagement and performance. The market size of Indian online education, driven by AI advancements, crossed $5 billion in 2023, highlighting rapid sector expansion.

AI’s deployment in urban development is exemplified by initiatives such as the Smart Cities Mission, under which AI manages traffic, enhances public safety and improves urban planning. Surat’s AI-driven integrated traffic management system uses data analytics and machine learning to optimise traffic flow, reduce congestion and enhance road safety, resulting in a 12 per cent reduction in traffic delays and a 20 per cent improvement in emergency response times.

NITI Aayog is partnering with C4IR India to create ethical AI frameworks addressing bias, accountability and transparency. This collaboration aims to deploy AI technologies respecting human rights and promoting social equity. For example, AI-based predictive policing systems in Hyderabad and Delhi were piloted with protocols to prevent discriminatory practices and ensure data privacy. Partnerships with countries like Japan, the US and Israel have facilitated knowledge exchange and technological advancements. India’s journey towards AI excellence involves a comprehensive and multifaceted approach. By integrating AI into various sectors, establishing robust policy frameworks, addressing ethical and societal concerns and fostering innovation through collaboration, India can harness the full potential of AI.",,
91,"Meta, Apple, Microsoft Expand AI Risk Transparency Amid Pressure",https://news.bloomberglaw.com/esg/meta-apple-microsoft-move-to-fend-off-mounting-ai-concerns,Awais,News Article,"Meta, Apple, Microsoft Expand AI Risk Transparency Amid Pressure Companies from social media giant Meta Platforms Inc. to software provider Microsoft Corp. are growing more transparent about their use of artificial intelligence amid pressure from government bodies, oversight committees and investors. And the shareholder campaigns are just getting started.

Meta recently updated its relatively-new AI labeling policy to be even clearer about content generated by the technology to tackle concerns about the potential spread of misinformation across Facebook and its other platforms. Microsoft released an inaugural responsible AI report in May. And Apple announced that it would disclose more about its AI plans after a proposal seeking more AI-related business and ethics information received 37.5% support from shareholders in February.

The businesses are among a half-dozen targets that shareholders have pressed to divulge the risk the AI tools they’re developing to remain competitive in their industries pose to their finances and operations, as well as to their employees and society more generally. In addition to technology companies, the entertainment industry has become a focus of shareholder efforts after use of the emerging technology galvanized labor concerns during last summer’s Hollywood strikes.

An AI bid at streaming company Netflix Inc. secured 43% of shareholder votes in June—a near-passing result that is very rare for a first-time effort.

The pressure is not going to stop companies from moving forward with AI. Businesses are banking on AI as a monumental financial opportunity and touting their AI focus in filings to investors: Bloomberg Law reported in February that over 40% of S&P 500 companies mentioned AI in their most recent annual report—an uptick since 2018 when AI was rarely mentioned.

The campaigns are, however, starting to prompt some businesses to modify their behavior. Earlier this year, the AFL-CIO said it withdrew AI-related bids at The Walt Disney Co. and Comcast Corp. after those companies reportedly agreed to disclose more information on the use of AI.

Pressure from the investors and others is going to continue to push companies across industries to divulge more information about their AI use, said Beena Ammanath, global and US technology trust ethics leader at Deloitte LLP.

“There is enough awareness now that we’re going to see that shift to be more transparent,” Ammanath said. “I get to speak to a lot of boards and CEOs and their leadership teams, and I can tell you that the level of awareness or activity that is happening at a board level—something like this hasn’t happened in a long time.”

Big Tech
Microsoft released its inaugural responsible AI report in May explaining how it builds generative AI systems to mitigate misinformation and disinformation. The tech giant committed to producing the report to the US government one year ago.

That commitment wasn’t enough to satisfy investors. Microsoft was the first of several companies to face a shareholder proposal late last year urging it to detail its AI risk and plans to remediate any potential harms. Even though it had already promised its responsible AI report for the US government, 21.2% of investors still supported the bid from Arjuna Capital in December asking the company’s board to produce an additional report.

“We believe Microsoft’s multi-faceted program to address the risks of misinformation and disinformation is longstanding and effective,” the company said in its proxy statement.

The height of AI investor pressure came at Alphabet Inc.’s June annual meeting when the parent company of Google faced three AI proposals at once—more than any other business so far this year. The proposals asked for a report on misinformation and disinformation spread by AI, which received 17.6% support; a governance shift to make the board’s audit and compliance committee responsible for overseeing Alphabet’s AI use, which received 7.4% backing; and a human rights assessment examining Google’s targeted advertising policies, which secured 18.6% of the investor vote.

Any result in the double digits is considered enough to potentially sway company behavior, even if the proposal does not pass. Alphabet’s AI products include the Gemini chatbot, formerly known as Bard, which can be used for writing, research and other language-related tasks. The tech giant told shareholders that it’s committed to “applying Alphabet’s resources responsibly as it continues to unlock the growth potential of AI across its products and services.”

Ultimately, investors want big tech to be more transparent and careful about how rapid AI development could pan out in the long run.

“How fast is too fast, and how much are you willing to sacrifice society for profit?” asked Jonas Kron, chief advocacy officer of Trillium Asset Management, which brought the AI governance proposal at Alphabet.

Meta updated its AI policy in July to warn users about manipulated media. The social media giant launched a “Made with AI” label in April, but it recently changed the tag to say “AI info” instead, which users can click to get more information. Meta said the update—which it originally rolled out after pressure from an independent oversight board that called for a revamp of its policies—is intended to provide more context, because the previous label wasn’t always aligned with users’ expectations.

Meta, which has launched a new digital assistant feature that can answer user questions and generate images, faced a proposal from Arjuna Capital in May that received 16.7% of investor support. Arjuna pointed out that the vote result was significant considering Mark Zuckerberg controls over half of the company’s voting power, and voted against the bid.

Arjuna is going to continue to urge more businesses to make changes. “The risks aren’t going away, so our engagements aren’t going away,” said Julia Cederholm, senior associate of ESG research and shareholder engagement at Arjuna.

Meta said in its proxy statement that it has already “made significant investments” in safety and security to tackle misinformation and disinformation.

Entertainment Pressure
The proposal that almost passed at Netflix raised concerns about potential hiring discrimination, mass layoffs and facility closures, and argued that ethical guidelines for AI use could help avoid labor disruptions. The investor effort followed entertainment industry worker concerns that AI could take credit from or replace writers or be used to replicate actors’ likenesses.

Netflix said in its proxy statement that it’s already subject to collective bargaining agreements with entertainment industry unions that include AI provisions. Netflix also said the type of report the proposal sought “may require disclosure of strategic initiatives, confidential research and development activities, and other information that may harm our competitive position.”

Carin Zelenko, director of capital strategies for the AFL-CIO, said the entertainment industry strikes demonstrated what happens when businesses don’t engage workers in thinking through how the use of technology could impact jobs and the future of the industry.

“I really believe it’s important that, as companies are introducing these technologies, that they engage the workforce in how the technology can be used,” Zelenko said.

Some employees feel the same. Ylonda Sherrod, an AT&T sales consultant in Ocean Springs, Mississippi, and a member of the Communications Workers of America, is speaking up about her AI concerns about worker empowerment and transparency.

“I feel like we should have a say in how it’s implemented in the workplace, because it could be implemented better,” she said in an interview, adding that there should be restrictions and policies in place to make workers feel more secure across industries.

No Fixed Playbook
As the AI race ramps up, companies are going to continue wrangling with how best to navigate ethical, legal and regulatory issues covering a range of topics from data privacy to the environmental impact of the technology.

AI risk is heightened by new rules like the EU’s AI Act that will take effect on Aug. 1. That law—like the shareholder proposals—aims to make sure AI systems are governed by safe and ethical principles. The Act bans what it deems “unacceptable risk” like using AI systems that could manipulate individuals or exploit them because of their age or disability, for example.

The law will apply to providers and developers of AI tools that are used in the EU even if the companies are based elsewhere.

The US has been slower to adopt any laws on AI use, but the White House issued an executive order late last year with sweeping security and privacy measures and other directives, including a requirement that developers share their safety test results with the US government. Securities and Exchange Commission Chair Gary Gensler also gave companies a stern warning in December about misleading investors about their AI capabilities, a phenomenon that he called “AI washing.”

With inconsistent and incomplete guidance from regulators so far, some companies are working to set up their own risk mitigation infrastructures.

“I think organizations are experimenting, right now there is no fixed playbook for it,” said Deloitte’s Ammanath.

Some companies have created new high-level roles to tackle the mammoth issue, including chief AI ethics officer, chief tech ethics officer, and even chief trust officer. They’re also setting up committees on AI or tech ethics either with internal or external members.

But as businesses divulge more about their AI plans, it’s important to be clear and tailor the release of information to the right stakeholders, Ammanath said.

“The way you communicate about or explain how a model works to a data scientist would be different to how you explain it to your board or customer or investor,” she said.",,
93,Original sins and dirty secrets: GenAI has an ethics problem. These are the three things it most urgently needs to fix,https://fortune.com/2024/06/27/gen-ai-ethics-original-sins-data-theft-labor-exploitation-environmental-impact/,Dan,News Article,"Original sins and dirty secrets: GenAI has an ethics problem. These are the three things it most urgently needs to fix.                                                                          The ethics of generative AI has been in the news this week. AI companies have been accused of taking copyrighted creative works without permission to train their models, and there’s been documentation of those models producing outputs that plagiarize from that training data. Today, I’m going to make the case that generative AI can never be ethical as long as three issues that are currently inherent to the technology remain. First, there’s the fact that generative AI was created using stolen data. Second, it’s built on exploitative labor. And third, it’s exponentially worsening the energy crisis at a pivotal time when we need to be scaling back, not accelerating, our energy demands and environmental impact.

The first unethical truth of generative AI has probably gotten the most attention. Over the past few years, there’s been a constant stream of lawsuits, open letters, and labor disputes over the technology from artists, writers, publishers, musicians, and other creatives whose work was used to train large language models. This issue and its impact on the publishing and music industries has been playing out this week after generative AI search engine Perplexity was caught scraping publishers’ content against their wishes (after it promised it wouldn’t) and the Recording Industry Association of America filed copyright infringement lawsuits against two well-funded AI companies, as my Eye on AI cowriter Jeremy Kahn covered thoroughly in Tuesday’s newsletter (and I have updates on the music story later). If you haven’t already read his essay, go and give it a read.

But what’s important to know beyond why these data practices are so unethical is just how deeply tied they are to generative AI as it exists today. AI companies hoovered up content en masse—including copyrighted works—to train their models because it was the only way they could build this technology that they were so invested in and so determined to build. The New York Times recently referred to this as “the original sin of AI” and reported on how OpenAI, for example, even built a tool specifically to transcribe the audio from podcasts, audiobooks, and YouTube videos into text in order to train ChatGPT.

AI companies are increasingly seeking out deals to license data to train models. It’s still not clear if this can solve the problem—or what new ones it might create. But agreeing to pay for training data now that they’re under fire certainly feels like an admission that they didn’t have the right to take it all along.

Now onto generative AI’s labor problem. These models yet again would not exist without the labor of workers—many of them in the Global South—who are paid little more than $1 per hour to make all the data that was scraped usable for AI models. This work sometimes involves labeling images, video footage, and other data, including content that is horrifically violent. In other cases, these data workers help refine the outputs of large language models, evaluating LLM outputs and guiding the models to be helpful and avoid racist, sexist, or other harmful outputs through a process known as “reinforcement learning through human feedback,” or RLHF. It is this process that takes a raw model, like GPT-4, and turns it into ChatGPT.

In an open letter to President Joe Biden I reported on last month, Kenyan AI data labelers who work for companies including Meta, OpenAI, and Scale AI said U.S. tech companies are systematically exploiting African workers and accused them of undermining local labor laws, comparing their working conditions to “modern day slavery.”

“Our work involves watching murder and beheadings, child abuse and rape, pornography and bestiality, often for more than 8 hours a day. Many of us do this work for less than $2 per hour,” they wrote.

Of course, this isn’t news. Reports of the exploitation of low-paid AI workers go back years, such as one in Rest of World—an excerpt from Phil Jones’s book Work Without the Worker— that in 2021 detailed how Big Tech companies have targeted populations devastated by war and economic collapse to perform the work: “Their nights are spent labeling footage of urban areas—‘house,’ ‘shop,’ ‘car’—labels that, in a grim twist of fate, map the streets where the labelers once lived, perhaps for automated drone systems that will later drop their payloads on those very same streets.” It also closely mirrors the content moderation practices of social media platforms, which have similarly come under fire for their miniscule pay and hostile work conditions.  

When AI executives and investors compare the technology to magic, they never mention these workers and the intense labor they perform to make it possible. If the taking of data without consent is generative AI’s original sin, the widespread labor exploitation is its dirty secret. 

Lastly, there’s the energy usage, water consumption, and sustainability problems of generative AI.

You’ve undoubtedly heard the dire climate warnings and seen coverage of record-breaking heat and countless climate disasters over the past few years. If you haven’t read about how the demands for AI’s growth-at-all-costs is wreaking havoc on the global energy supply, pushing grids to the brink, guzzling up massive amounts of water, and threatening to upend the energy transition plans needed to ward off the worst effects of climate change, Bloomberg and Vox both had great stories on the topic this past weekend. 

Datacenters are already using more energy than entire countries—and AI is behind much of the recent growth in their power consumption—a trend that’s only expected to get worse as companies race to create larger AI models trained on more and more increasingly powerful chips. The size of AI datacenters is ballooning, using 10 to 15 times the amount of electricity, one energy executive told Bloomberg. Nvidia’s next-generation GPU for generative AI, the B100, can consume nearly twice as much power as the H100 that is used for today’s leading models. OpenAI CEO Sam Altman has said we need an energy breakthrough for AI, and even Microsoft has admitted AI is jeopardizing its long-term goal to be carbon-negative by 2030. Projections for the future of AI’s energy usage are bleak: In the U.K., for example, AI is expected to increase energy demands by 500% over the next decade.

“I’m sure you’ve read about these 100,000 GPU supercomputers that people want to build, all of the money that’s going into them, the power constraints, the environmental and water constraints. I’ve not been able to stop thinking about how potentially unsustainable [it is],” Chris Kauffman, a principal investor at VC firm General Catalyst, which is invested in AI companies including Adept and Mistral, told me.

These issues don’t apply to all types of AI models. But it’s also not a comprehensive list of AI’s ethical issues. For instance, bias in facial recognition systems and models used to convict people of crimes, deny them loans and insurance, and decide who gets job interviews and admitted into colleges are another dimension of how the way in which AI models are built, and the way in which we are using them, is failing to live up to our ethical ideals.

AI companies say they’re committed to developing AI responsibly. They’ve signed a bunch of (non-binding) pledges to do so. These practices, however, suggest otherwise. 

",,
98,"Friar Tech: The Vatican's top AI ethics expert who advises Pope Francis, the UN and Silicon Valley",https://www.euronews.com/next/2024/01/21/friar-tech-the-vaticans-top-ai-ethics-expert-who-advises-pope-francis-the-un-and-silicon-v,Awais,News Article,"Friar Tech: The Vatican's top AI ethics expert who advises Pope Francis, the UN and Silicon Valley      It might seem like an oxymoron but the Vatican's top brain on the technology most shaping our world just now hails from a medieval order.

Friar Paolo Benanti wears the plain brown robes of his medieval Franciscan order as he pursues one of the most pressing issues in contemporary times: how to govern artificial intelligence (AI) so that it enriches - and doesn’t exploit - people's lives.

Benanti is the Vatican's go-to person on the technology and he has the ear of Pope Francis as well as some of Silicon Valley's top engineers and executives.

With a background in engineering, a doctorate in moral theology, and a passion for what he calls the ""ethics of technology,'' the 50-year-old Italian priest is on an urgent mission that he shares with Francis, who, in his annual peace message for 2024, pushed for an international treaty to ensure the ethical use of AI technology.  ""What is the difference between a man who exists and a machine that functions?"" said Benanti in an interview with The Associated Press during a break at the Pontifical Gregorian University, where he teaches courses such as moral theology and bioethics to students preparing for the priesthood. 

""This is perhaps the greatest question of these times, because we are witnessing a challenge that every day grows more profound with a machine that is humanising"".

Adviser on AI
Benanti is a member of the United Nations' Advisory Body on Artificial Intelligence as well as head of an Italian government commission tasked with providing recommendations on how to safeguard journalism from fake news and other disinformation. 

He is also a consultant to the Vatican’s Pontifical Academy for Life. Benanti says he helps ""better clarify the more technical terms for the Holy Father"" during their encounters.

His knowledge came in handy for a 2023 meeting at the Vatican between Francis and Microsoft President Brad Smith that focused on how AI could help or hurt humanity.

Francis and Smith had also discussed artificial intelligence ""at the service of the common good"" during a meeting a few years earlier, according to the Vatican.

With a papacy heavily attentive to those who live on society’s margins, Francis has made clear his concern that AI technology could limit human rights by, say, negatively impacting a homebuyer's mortgage application, a migrant’s asylum bid, or an evaluation of an offender’s likelihood to repeat a crime.

""It’s clear that if we choose some data that aren’t sufficiently inclusive, we will have some choices that aren’t inclusive,"" said Benanti, whose religious order was founded in the early 13th century by St Francis of Assisi, who renounced earthly riches and promoted charitable works.
Microsoft first reached out to Benanti several years ago for his thoughts on technology, the friar said. Benanti noted that much of the data that informs AI is fed by low-wage workers, many in developing countries entrenched in a history of colonialism and an exploited workforce.

""I don’t want this to be remembered as the season in which we extract from the global South cognitive resources,"" he said. 

If one examines ""the best tools that we are producing in AI"" in the West, one sees that AI is ""trained with underpaid workers from English-speaking former colonies"".

How to govern AI is an issue that countries all over the globe are trying to resolve. The European Union became a trailblazer late last year when negotiators secured a deal that paves the way for legal oversight of AI technology.

In Italy, premier Giorgia Meloni, who worries that AI could lead to job losses, will make the technology a focus of this year's G-7 summit being hosted by Italy.

In 2023, Smith did a podcast with Benanti in Rome, describing the friar as bringing ""one of the most fascinating combinations in the world"" in terms of his background in engineering, ethics, and technology, to the AI debate.

Finding the right uses of AI
Benanti, who was one year shy of obtaining his engineering degree at Rome's Sapienza University when he forsook the degree - and also his girlfriend - to join the Franciscans in his 20s, described how AI could be a ""really powerful tool"" in bringing down the cost of medicine and empowering doctors to help more people.

But he also described the ethical implications of a technology that could have the same capabilities as a human - or perhaps even more.

""It is a problem not of using [AI] but it is a problem of governance,'' the friar said. ""And here is where ethics come in - finding the right level of use inside a social context"". As part of those efforts, Meloni on Thursday met with visiting Microsoft founder Bill Gates in Rome, an encounter that was attended by Benanti.

For his part, the friar told AP that regulating artificial intelligence shouldn't mean limiting its development.

""It means keeping them compatible with that fragile system that is democracy, that today seems to be the best system,"" Benanti said.


",,
